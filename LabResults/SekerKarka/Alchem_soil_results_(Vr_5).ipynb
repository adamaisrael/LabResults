{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***מחברת קוד לעיבוד נתוני תוצאות אלכם - סקר קרקע***\n",
        "\n",
        "<bdi>\n",
        "\n",
        "ברירת המחדל של הטבלה הינה הצגת חריגות מערך ה-VSL.\n",
        "\n",
        "ניתן להציג חריגות מערך סף נוסף בהתאם למאפייני האתר.\n",
        "\n",
        "בתא השני נבחר את התרכובות אותם נרצה להציג בטבלת התוצאות. לדוגמה: TPH, MTBE BTEX וכו'.\n",
        "\n",
        "סימון האופציה **Add_exceptations** יציג בטבלה את כל תרכובות ה-VOC בהם יש חריגות מערכי הסף הנבחרים, VSL ו-TIER 1.\n",
        "\n",
        "ב-**Add_specifics** נוסיף מספרי CAS של תרכובות נוספות אותן נרצה להציג על אף שלא אותרו בהן חריגות מערכי הסף.\n",
        "\n",
        "\n",
        "על מנת לעשות זאת, יש לרשום במקום המיועד לכך לפי הפורמט הבא:\n",
        "\n",
        "'cas number, cas number'\n",
        "\n",
        " לדוגמה: '106-93-4, 74-83-9'\n",
        "\n",
        " ניתן הוסיף כמה אנליזות שרוצים, 1,2,3 וכו'.\n",
        "\n",
        " אם תבוקש אנליזה שלא נמצאת בתוצאות מעבדה תקפוץ הודעה, יש ללחוץ אישור ולבדוק איזה אנליזה לא באמת קיימת בקובץ תוצאות שהעלתם\n",
        "\n",
        "</bdi>"
      ],
      "metadata": {
        "id": "gAKKipm41-F1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4dpFJP75WgVI",
        "outputId": "f0ca7a8e-fcfb-4db0-e660-8f6c4a9fb39e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "#@title נא לבחור פרמטרים מתאימים  { run: \"auto\" }\n",
        "\n",
        "Tier1_type = \"Tier 1 industrial A 0-6m\" # @param [\"Tier 1 Residential A 0-6m\",\"Tier 1 Residential A >6m\", \"Tier 1 Residential B\",\"Tier 1 industrial A 0-6m\",\"Tier 1 industrial A >6m\",\"Tier 1 industrial B\"]\n",
        "\n",
        "testsׁ_basic = \"Metals\" # @param [\"TPH\",\"MTBE BTEX\",\"TPHּּּ + MTBE BTEX\", \"Metals\", \"TPH + Metals\", \"MTBE BTEX + Metals\", \"TPHּּּ + MTBE BTEX + Metals\"]\n",
        "\n",
        "Add_exceptations = False # @param {type:\"boolean\"}\n",
        "\n",
        "Add_specifics = None # @param {type:\"raw\"}\n",
        "\n",
        "file_name = 'תוצאות אשכול מתכות סופי' # @param {type:\"string\"}\n",
        "\n",
        "Font_size_hebrew = 9 # @param {type:\"slider\", min:6, max:24, step:1}\n",
        "\n",
        "Font_name_hebrew = 'David' # @param {type:\"string\"}\n",
        "\n",
        "Font_size_english = 8 # @param {type:\"slider\", min:6, max:24, step:1}\n",
        "\n",
        "Font_name_english = 'Times New Roman' # @param {type:\"string\"}\n",
        "\n",
        "!pip install openpyxl\n",
        "from openpyxl.styles import Font\n",
        "import pandas as pd\n",
        "import re\n",
        "from IPython.display import Javascript\n",
        "\n",
        "if Add_specifics == None:\n",
        "  Add_specifics = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title להעלות קובץ ערכי סף של קרקע בלחיצה על הכפתור למטה\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "filenames = list(uploaded.keys())\n",
        "\n",
        "# Initialize an empty list to store the dataframes\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file and read it based on its extension\n",
        "for filename in filenames:\n",
        "    if filename.endswith('.csv'):\n",
        "        df = pd.read_csv(filename)\n",
        "    elif filename.endswith('.xlsx'):\n",
        "        df = pd.read_excel(filename)\n",
        "    else:\n",
        "        print(f\"Unsupported file format: {filename}\")\n",
        "        continue\n",
        "        # Round all numeric cells to 2 decimal points\n",
        "    #df = df.applymap(lambda x: round(x, 2) if isinstance(x, (int, float)) else x)\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all dataframes into one\n",
        "if dataframes:\n",
        "    threshold_data = pd.concat(dataframes, ignore_index=True)\n",
        "else:\n",
        "    print(\"No valid files uploaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "cellView": "form",
        "id": "c3tohGf-rAhP",
        "outputId": "245c4c05-03c6-4d3a-a5da-48dcd4f374cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-56c01bd1-0579-4d23-97cb-6a7d12d03a07\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-56c01bd1-0579-4d23-97cb-6a7d12d03a07\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving גרסה 7 דצמבר 2024 - soil_threshold.csv to גרסה 7 דצמבר 2024 - soil_threshold (3).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title להעלות קובץ טבלת תוצאות של קרקע בלחיצה על הכפתור למטה\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# List of uploaded filenames\n",
        "filenames = list(uploaded.keys())\n",
        "\n",
        "# Initialize empty lists to store dataframes for TPH, IPC, and VOC splits\n",
        "dataframes_TPH = []\n",
        "dataframes_ICP = []\n",
        "dataframes_VOC_splits = []\n",
        "\n",
        "# Loop through each file and process based on extension\n",
        "for filename in filenames:\n",
        "    if filename.endswith('.xlsx'):\n",
        "        # Read the TPH sheet\n",
        "        try:\n",
        "            df_TPH = pd.read_excel(filename, sheet_name='TPH')\n",
        "            # Round numeric cells to 2 decimal points\n",
        "            #df_TPH = df_TPH.applymap(lambda x: round(x, 2) if isinstance(x, (int, float)) else x)\n",
        "            dataframes_TPH.append(df_TPH)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading TPH sheet of {filename}: {e}\")\n",
        "\n",
        "        # Read the IPC sheet if it exists\n",
        "        try:\n",
        "            df_ICP = pd.read_excel(filename, sheet_name='ICP', header=None)\n",
        "            # Append the DataFrame to IPC list\n",
        "            dataframes_ICP.append(df_ICP)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading ICP sheet of {filename}: {e}\")\n",
        "\n",
        "        # Read the VOC sheet and process \"Analysis Location:\" splitting\n",
        "        try:\n",
        "            # Read the VOC sheet without treating the first row as headers\n",
        "            df_VOC = pd.read_excel(filename, sheet_name='VOC', header=None)\n",
        "\n",
        "            # Identify rows containing \"Analysis Location:\"\n",
        "            analysis_location_rows = df_VOC[df_VOC.apply(\n",
        "                lambda row: row.astype(str).str.contains(\"Analysis Location:\").any(), axis=1)].index.tolist()\n",
        "\n",
        "            # Count occurrences and split based on indices\n",
        "            if len(analysis_location_rows) > 0:\n",
        "                for i in range(len(analysis_location_rows)):\n",
        "                    start_row = analysis_location_rows[i]\n",
        "                    end_row = analysis_location_rows[i + 1] if i + 1 < len(analysis_location_rows) else None\n",
        "                    split_df = df_VOC.iloc[start_row:end_row]\n",
        "                    dataframes_VOC_splits.append(split_df.reset_index(drop=True))\n",
        "            else:\n",
        "                # If no \"Analysis Location:\", treat the entire sheet as one DataFrame\n",
        "                dataframes_VOC_splits.append(df_VOC.reset_index(drop=True))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading VOC sheet of {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Unsupported file format: {filename}\")\n",
        "\n",
        "# Concatenate all TPH sheet dataframes into one\n",
        "if dataframes_TPH:\n",
        "    results_data_TPH = pd.concat(dataframes_TPH, ignore_index=True)\n",
        "else:\n",
        "    print(\"No valid TPH sheets found.\")\n",
        "    results_data_TPH = pd.DataFrame()\n",
        "\n",
        "# Concatenate all IPC sheet dataframes into one\n",
        "if dataframes_ICP:\n",
        "    results_data_ICP = pd.concat(dataframes_ICP, axis=1)\n",
        "else:\n",
        "    print(\"No valid IPC sheets found.\")\n",
        "    results_data_ICP = pd.DataFrame()\n",
        "\n",
        "# Concatenate all VOC splits horizontally, dropping the first 5 columns from the 2nd DataFrame onward\n",
        "if dataframes_VOC_splits:\n",
        "    # Keep the first 5 columns only from the first DataFrame\n",
        "    processed_dfs = [dataframes_VOC_splits[0]]\n",
        "    processed_dfs += [df.iloc[:, 5:].reset_index(drop=True) for df in dataframes_VOC_splits[1:]]\n",
        "\n",
        "    # Align and concatenate by rows\n",
        "    results_data_VOC_horizontal = pd.concat(processed_dfs, axis=1)\n",
        "else:\n",
        "    print(\"No valid VOC splits found.\")\n",
        "    results_data_VOC_horizontal = pd.DataFrame()\n",
        "\n",
        "results_data_VOC = results_data_VOC_horizontal\n",
        "\n",
        "# Convert the data to string to avoid errors\n",
        "results_data_TPH = results_data_TPH.astype(str)\n",
        "results_data_ICP = results_data_ICP.astype(str)\n",
        "results_data_VOC = results_data_VOC.astype(str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "UdxL50sk7Jcg",
        "outputId": "8d0fdd0b-803f-4792-87c7-471e37fe4d4d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8bb50d7d-86eb-467b-a037-f0853b072a7e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8bb50d7d-86eb-467b-a037-f0853b072a7e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving table737449B-980.xlsx to table737449B-980 (17).xlsx\n",
            "Saving table737452B-80.xlsx to table737452B-80 (17).xlsx\n",
            "Saving table737485B-238.xlsx to table737485B-238 (17).xlsx\n",
            "Saving table737497B-174.xlsx to table737497B-174 (17).xlsx\n",
            "Saving table737549B-605.xlsx to table737549B-605 (12).xlsx\n",
            "Saving עותק של table737550B-535.xlsx to עותק של table737550B-535 (21).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title יש להריץ את התא ולהוריד את הטבלה המוכנה\n",
        "\n",
        "# Function to remove '(' and ')' from strings\n",
        "def remove_parentheses(cell):\n",
        "    if isinstance(cell, str):  # Only apply to strings\n",
        "        return cell.replace('(', '').replace(')', '')\n",
        "    return cell\n",
        "\n",
        "# Apply to the entire dataframe for results_data_TPH\n",
        "results_data_TPH = results_data_TPH.applymap(remove_parentheses)\n",
        "\n",
        "# Apply to the entire dataframe for results_data_VOC\n",
        "results_data_VOC = results_data_VOC.applymap(remove_parentheses)\n",
        "\n",
        "# Apply to the entire dataframe for results_data_ICP\n",
        "results_data_ICP = results_data_ICP.applymap(remove_parentheses)\n",
        "\n",
        "drills_list = results_data_TPH[\"Analyte\"].dropna().tolist()\n",
        "drills_list = drills_list[1:]\n",
        "\n",
        "# Function to replace \"N.D.\" values\n",
        "def replace_nd(cell, column):\n",
        "    if cell == \"N.D.\":\n",
        "        return \"<15\" if column == \"Total TPH\" else \"<7\"\n",
        "    return cell\n",
        "\n",
        "# Apply the replacement logic to the dataframe\n",
        "results_data_TPH = results_data_TPH.apply(\n",
        "    lambda col: col.apply(lambda cell: replace_nd(cell, col.name))\n",
        ")\n",
        "\n",
        "voc_analysis_row = results_data_VOC[results_data_VOC.apply(\n",
        "    lambda row: row.astype(str).str.contains(\"Analysis Location:\").any(), axis=1)]\n",
        "\n",
        "if not voc_analysis_row.empty:\n",
        "    # Extract the row values as a list\n",
        "    voc_drills_list = voc_analysis_row.iloc[0].dropna().tolist()  # Drop NaN values\n",
        "\n",
        "    voc_drills_list = voc_drills_list[5:]\n",
        "\n",
        "    for voc_item in voc_drills_list:\n",
        "        # Check if voc_item is in any item of drills_list\n",
        "        if not any(voc_item in drill_item for drill_item in drills_list):\n",
        "            drills_list.append(voc_item)  # Add to drills_list if no match is found\n",
        "\n",
        "icp_analysis_row = results_data_ICP[results_data_ICP.apply(\n",
        "    lambda row: row.astype(str).str.contains(\"Sample Name\").any(), axis=1)]\n",
        "\n",
        "if not icp_analysis_row.empty:\n",
        "    if testsׁ_basic == \"Metals\":\n",
        "      drills_list = []\n",
        "\n",
        "    # Extract the row values as a list\n",
        "    icp_drills_list = icp_analysis_row.iloc[0].dropna().tolist()  # Drop NaN values\n",
        "\n",
        "    icp_drills_list = icp_drills_list[5:]\n",
        "\n",
        "    for icp_item in icp_drills_list:\n",
        "\n",
        "        # Check if voc_item is in any item of drills_list\n",
        "        if not any(icp_item in drill_item for drill_item in drills_list) and \"Metals\" in testsׁ_basic:\n",
        "            drills_list.append(icp_item)  # Add to drills_list if no match is found\n",
        "\n",
        "# Define the structure of the DataFrame\n",
        "columns = ['name', 'date', 'Unnamed: 2', 'depth', ' PID','full name']\n",
        "data = [\n",
        "    ['units', None, None, 'm', 'ppm', None],\n",
        "    ['Cas', None, None, None, None, None],\n",
        "    ['VSL', None, None, None, None, None],\n",
        "    ['TIER 1', None, None, None, None, None]\n",
        "]\n",
        "\n",
        "# # Normalize 'dup', 'duplicate', or 'Duplicate' to 'DUP'\n",
        "# normalized_drills_list = []\n",
        "# for item in drills_list:\n",
        "#     # Replace 'dup', 'duplicate', or 'Duplicate' with 'DUP' (case-insensitive)\n",
        "#     normalized_item = re.sub(r'\\b(?:dup|duplicate|Duplicate)\\b', 'DUP', item, flags=re.IGNORECASE)\n",
        "#     normalized_drills_list.append(normalized_item)\n",
        "\n",
        "# # Remove duplicates by converting to a set, then back to a list to maintain unique items\n",
        "# normalized_drills_list = list(set(normalized_drills_list))\n",
        "\n",
        "# # Sort the list if you want to keep it in a specific order\n",
        "# normalized_drills_list.sort()\n",
        "# drills_list = normalized_drills_list\n",
        "\n",
        "# Create the DataFrame\n",
        "df_template = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# # Process each item in drills_list\n",
        "# df_template['full name'] = None\n",
        "# for item in drills_list:\n",
        "#     # Find the first instance of a number with a dot and numbers on both sides\n",
        "#     match = re.search(r'(\\d+\\.\\d+)', item)\n",
        "#     if match:\n",
        "#         # Extract the depth (value) and the name (key)\n",
        "#         depth = match.group(1)  # The matched number with a dot\n",
        "#         name = item[:match.start()-1].strip()  # Get everything before the depth\n",
        "#         new_row = {'name': name, 'depth': depth, 'full name' : item}  # Create a new row with name and depth\n",
        "#         df_template = pd.concat([df_template, pd.DataFrame([new_row])], ignore_index=True)\n",
        "# # Fill missing columns with None to match the template structure\n",
        "# df_template = df_template.reindex(columns=columns, fill_value=None)\n",
        "\n",
        "# # Create the DataFrame\n",
        "# df_template = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "\n",
        "# Process each item in drills_list\n",
        "df_template['full name'] = None\n",
        "\n",
        "# Updated extraction logic for name, depth, and type\n",
        "def extract_name_and_depth(item):\n",
        "    if not isinstance(item, str):\n",
        "        return item, None, None\n",
        "\n",
        "    # Detect flags\n",
        "    is_dup = bool(re.search(r'\\b(?:dup|duplicate|DUP|D)\\b', item, re.IGNORECASE))\n",
        "    is_split = bool(re.search(r'\\b(?:split|SP)\\b', item, re.IGNORECASE))\n",
        "\n",
        "    # Clean flags from item for name extraction\n",
        "    clean_item = re.sub(r'\\b(?:dup|duplicate|DUP|D|split|SP)\\b', '', item, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Match name and depth\n",
        "    match_paren = re.search(r'(.+?)\\s*\\((\\d+\\.\\d+)\\)', clean_item)\n",
        "    if match_paren:\n",
        "        name = match_paren.group(1).strip()\n",
        "        depth = match_paren.group(2)\n",
        "    else:\n",
        "        match_dash = re.search(r'^(.+?-\\d+\\.\\d+)\\s*-\\s*(\\d+\\.\\d+)', clean_item)\n",
        "        if match_dash:\n",
        "            name = match_dash.group(1).strip()\n",
        "            depth = match_dash.group(2)\n",
        "        else:\n",
        "            match = re.findall(r'(\\d+\\.\\d+)', clean_item)\n",
        "            if match:\n",
        "                depth = match[-1]\n",
        "                name = clean_item[:clean_item.rfind(depth)].strip(\" -\")\n",
        "            else:\n",
        "                name, depth = clean_item.strip(), None\n",
        "\n",
        "    return name, depth, ('**' if is_split else '*') if is_dup or is_split else ''\n",
        "\n",
        "# Build a temp list to group regular, DUP, and split rows\n",
        "temp_rows = {}\n",
        "\n",
        "for item in drills_list:\n",
        "    name, depth, suffix = extract_name_and_depth(item)\n",
        "    if depth is None:\n",
        "        continue\n",
        "    key = (name, depth)\n",
        "\n",
        "    # Initialize group\n",
        "    if key not in temp_rows:\n",
        "        temp_rows[key] = []\n",
        "\n",
        "    # Compose adjusted depth\n",
        "    depth_display = f\"{depth}{suffix}\"\n",
        "    temp_rows[key].append({\n",
        "        'name': name,\n",
        "        'depth': depth_display,\n",
        "        'full name': item\n",
        "    })\n",
        "\n",
        "# Flatten into df_template in the order: original, DUP, split\n",
        "for key in sorted(temp_rows.keys()):\n",
        "    # Ensure order: normal -> * -> **\n",
        "    ordered = sorted(temp_rows[key], key=lambda x: len(x['depth']))\n",
        "    for row_data in ordered:\n",
        "        new_row = {**row_data}\n",
        "        df_template = pd.concat([df_template, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "# Ensure columns match\n",
        "df_template = df_template.reindex(columns=columns, fill_value=None)\n",
        "\n",
        "\n",
        "# Define the starting row for sorting\n",
        "start_row = 4\n",
        "\n",
        "# Extract the header rows (rows before start_row) and the rows to be sorted\n",
        "header_rows = df_template.iloc[:start_row]\n",
        "rows_to_sort = df_template.iloc[start_row:]\n",
        "\n",
        "# Extract base name (e.g., ק) and numeric part as float (e.g., 1.1 from ק-1.1)\n",
        "rows_to_sort['name_prefix'] = rows_to_sort['name'].str.extract(r'([^-\\d]+)', expand=False)\n",
        "rows_to_sort['name_number'] = rows_to_sort['name'].str.extract(r'(\\d+\\.\\d+|\\d+)', expand=False).astype(float)\n",
        "\n",
        "# Sort by prefix, then by numeric name, then by depth\n",
        "sorted_rows = rows_to_sort.sort_values(\n",
        "    by=['name_prefix', 'name_number', 'depth'],\n",
        "    key=lambda col: pd.to_numeric(col, errors='coerce') if col.name in ['name_number', 'depth'] else col\n",
        ")\n",
        "\n",
        "# Drop the temporary columns used for sorting\n",
        "sorted_rows = sorted_rows.drop(columns=['name_prefix', 'name_number'])\n",
        "\n",
        "# Concatenate the header rows with the sorted rows\n",
        "df_sorted = pd.concat([header_rows, sorted_rows], ignore_index=True)\n",
        "\n",
        "#####סידור תוצאות נדיפים######\n",
        "results_data_VOC_op = results_data_VOC.T\n",
        "\n",
        "# Replace \"<MDL\" and \"<MRL\" based on values in rows 2 and 3\n",
        "def replace_mdl_mrl(cell, column):\n",
        "    if isinstance(cell, str) and \"<MDL\" in cell:\n",
        "        return f\"<{results_data_VOC_op.iloc[2, column]}\"\n",
        "    elif isinstance(cell, str) and \"<MRL\" in cell:\n",
        "        return f\"<{results_data_VOC_op.iloc[3, column]}\"\n",
        "    return cell\n",
        "\n",
        "# Iterate over each column and apply the replacement\n",
        "for col in results_data_VOC_op.columns[2:]:  # Skip first two columns which are metadata\n",
        "    results_data_VOC_op[col] = results_data_VOC_op[col].apply(lambda x: replace_mdl_mrl(x, results_data_VOC_op.columns.get_loc(col)))\n",
        "\n",
        "# Remove rows 0, 2, 3, and 4 and column 1\n",
        "rows_to_remove = [0, 2, 3, 4]\n",
        "results_data_VOC_op = results_data_VOC_op.drop(index=rows_to_remove).reset_index(drop=True)\n",
        "results_data_VOC_op = results_data_VOC_op.drop(columns=results_data_VOC_op.columns[1])\n",
        "\n",
        "#####סידור תוצאות מתכות######\n",
        "if not results_data_ICP.empty:\n",
        "  results_data_ICP_op = results_data_ICP.T\n",
        "\n",
        "  # Replace \"N.D.\" and \"<LOQ\" based on values in rows 2 and 3\n",
        "  def replace_nd_loq(cell, column):\n",
        "      if isinstance(cell, str) and \"N.D.\" in cell:\n",
        "          return f\"<{results_data_ICP_op.iloc[1, column]}\"\n",
        "      elif isinstance(cell, str) and \"<LOQ\" in cell:\n",
        "          return f\"<{results_data_ICP_op.iloc[2, column]}\"\n",
        "      return cell\n",
        "\n",
        "  # Iterate over each column and apply the replacement\n",
        "  for col in results_data_ICP_op.columns[2:]:  # Skip first two columns which are metadata\n",
        "      results_data_ICP_op[col] = results_data_ICP_op[col].apply(lambda x: replace_nd_loq(x, results_data_ICP_op.columns.get_loc(col)))\n",
        "\n",
        "  # Remove rows 0, 2, 3, and 4 and column 1\n",
        "  rows_to_remove = [1, 2, 3]\n",
        "  results_data_ICP_op = results_data_ICP_op.drop(index=rows_to_remove).reset_index(drop=True)\n",
        "  results_data_ICP_op = results_data_ICP_op.drop(columns=results_data_ICP_op.columns[1])\n",
        "\n",
        "  metals_dic = {'Ag' : '7440-22-4', 'Al':'7429-90-5' , 'As' : '7440-38-2',\n",
        "  'Ba' : '7440-39-3', 'Be' : '7440-41-7', 'Cd' : '7440-43-9',\n",
        "  'Co' : '7440-48-4', 'Cr' : '7440-47-3', 'Cu' : '7440-50-8',\n",
        "  'Fe' : '7439-89-6', 'Hg' : '7439-97-6', 'Li' : '7439-93-2',\n",
        "  'Mn' : '7439-96-5', 'Mo' : '7439-98-7', 'Ni' : '7440-02-0',\n",
        "  'Pb' : '7439-92-1', 'Sb' : '7440-36-0', 'Se' : '7782-49-2',\n",
        "  'Tl' : '7440-28-0', 'V' : '7440-62-2', 'Zn' : '7440-66-6'}\n",
        "\n",
        "  # Replace row 0 values in results_data_ICP_op based on metals_dic\n",
        "  results_data_ICP_op.iloc[0] = results_data_ICP_op.iloc[0].map(metals_dic).fillna(results_data_ICP_op.iloc[0])\n",
        "\n",
        "  results_data_VOC_op.iloc[0, 0] = 'Sample Name'\n",
        "  result_outer = pd.merge(results_data_VOC_op, results_data_ICP_op, on=0, how='outer')\n",
        "\n",
        "  # Define the custom order based on sorted_rows['full name']\n",
        "  custom_order = sorted_rows['full name']\n",
        "\n",
        "  # Ensure column 0 in result_outer is treated as a categorical variable with the custom order\n",
        "  result_outer[0] = pd.Categorical(result_outer[0], categories=custom_order, ordered=True)\n",
        "\n",
        "  # Sort the DataFrame by column 0\n",
        "  result_outer_sorted = result_outer.sort_values(by=0)\n",
        "\n",
        "  # Reset the index if needed\n",
        "  result_outer_sorted.reset_index(drop=True, inplace=True)\n",
        "  result_outer_sorted = pd.concat([result_outer_sorted.iloc[[-1]], result_outer_sorted.iloc[:-1]]).reset_index(drop=True)\n",
        "\n",
        "  results_data_ICP_op = results_data_ICP_op.replace(\"Sample Name\", \"\")\n",
        "  results_data_VOC_op = result_outer_sorted\n",
        "  results_data_VOC_op.columns = range(len(results_data_VOC_op.columns))\n",
        "\n",
        "else:\n",
        "  print('ICP sheet is empty')\n",
        "\n",
        "########הכנסת נתונים לטמפלייט########\n",
        "if 'TPH' in testsׁ_basic:\n",
        "  df_sorted['TPH DRO'] = None\n",
        "  df_sorted['TPH ORO'] = None\n",
        "  df_sorted['Total TP'] = None\n",
        "\n",
        "  # Ensure column names are consistent and merge based on \"full name\" and \"Analyte\"\n",
        "  merged_df = df_sorted.merge(\n",
        "      results_data_TPH.rename(columns={\"Analyte\": \"full name\"}),\n",
        "      on=\"full name\",\n",
        "      how=\"left\"\n",
        "  )\n",
        "\n",
        "  # Update the relevant columns in df_sorted with data from results_data_TPH\n",
        "  df_sorted['TPH DRO'] = merged_df['DRO']\n",
        "  df_sorted['TPH ORO'] = merged_df['ORO']\n",
        "  df_sorted['Total TP'] = merged_df['Total TPH']\n",
        "\n",
        "  # Renaming the column 'Total TP' to 'Total TPH'\n",
        "  df_sorted.rename(columns={'Total TP': 'Total TPH'}, inplace=True)\n",
        "\n",
        "  #Adding CAS number\n",
        "  df_sorted['TPH DRO'][1] = \"C10-C40\"\n",
        "  df_sorted['TPH ORO'][1] = \"C10-C40\"\n",
        "  df_sorted['Total TPH'][1] = \"C10-C40\"\n",
        "\n",
        "if 'MTBE' in testsׁ_basic:\n",
        "  df_sorted['MTBE'] = None\n",
        "  df_sorted['Benzene'] = None\n",
        "  df_sorted['Toluene'] = None\n",
        "  df_sorted['Ethylbenzene'] = None\n",
        "  df_sorted['Xylene'] = None\n",
        "\n",
        "  #Adding CAS number\n",
        "  df_sorted['MTBE'][1] = '1634-04-4'\n",
        "  df_sorted['Benzene'][1] = '71-43-2'\n",
        "  df_sorted['Toluene'][1] = '108-88-3'\n",
        "  df_sorted['Ethylbenzene'][1] = '100-41-4'\n",
        "  df_sorted['Xylene'][1] = '95-47-6'\n",
        "\n",
        "\n",
        "\n",
        "  # Ensure matching names are correctly formatted\n",
        "  df_sorted['full name'] = df_sorted['full name'].astype(str).str.strip()\n",
        "  results_data_VOC_op[0] = results_data_VOC_op[0].astype(str).str.strip()\n",
        "\n",
        "  # Extract CAS mappings for the required columns\n",
        "  cas_map = df_sorted.loc[1, [\"MTBE\", \"Benzene\", \"Toluene\", \"Ethylbenzene\", \"Xylene\"]].to_dict()\n",
        "\n",
        "  # Iterate over each row in df_sorted and fill in the values\n",
        "  for index, row in df_sorted.iterrows():\n",
        "      if index <= 1:  # Skip non-data rows\n",
        "          continue\n",
        "      name = row['full name']\n",
        "      if pd.isna(name):\n",
        "          continue\n",
        "\n",
        "      # Find the matching name in results_data_VOC_op (reversed logic)\n",
        "      matching_row = results_data_VOC_op[results_data_VOC_op[0].apply(lambda x: x == name if pd.notna(x) else False)]\n",
        "      if matching_row.empty:\n",
        "          continue\n",
        "\n",
        "      for column, cas in cas_map.items():\n",
        "          if pd.isna(cas):\n",
        "              continue\n",
        "\n",
        "          # Find the column in results_data_VOC_op that matches the CAS\n",
        "          try:\n",
        "              cas_column_index = results_data_VOC_op.iloc[0, :].tolist().index(cas)  # Exact CAS match in the first row\n",
        "              value_to_fill = matching_row.iloc[0, cas_column_index]  # Extract the correct value\n",
        "              df_sorted.at[index, column] = value_to_fill\n",
        "          except ValueError:\n",
        "              # CAS not found in the first row\n",
        "              continue\n",
        "# Adding metals to df_sorted if 'Metals' is in tests_basic\n",
        "if 'Metals' in testsׁ_basic:\n",
        "    # Initialize columns for each metal in df_sorted\n",
        "    for metal, cas in metals_dic.items():\n",
        "        df_sorted[metal] = None\n",
        "\n",
        "    # Add CAS numbers in the second row (index 1) for each metal\n",
        "    for metal, cas in metals_dic.items():\n",
        "        df_sorted.at[1, metal] = cas\n",
        "\n",
        "    # Ensure matching names are correctly formatted\n",
        "    df_sorted['full name'] = df_sorted['full name'].astype(str).str.strip()\n",
        "    results_data_ICP_op[0] = results_data_ICP_op[0].astype(str).str.strip()\n",
        "\n",
        "    # Iterate over each row in df_sorted and fill in the values\n",
        "    for index, row in df_sorted.iterrows():\n",
        "        if index <= 1:  # Skip non-data rows\n",
        "            continue\n",
        "        name = row['full name']\n",
        "        if pd.isna(name):\n",
        "            continue\n",
        "\n",
        "        # Find the matching name in results_data_ICP_op\n",
        "        matching_row = results_data_ICP_op[results_data_ICP_op[0].apply(lambda x: x == name if pd.notna(x) else False)]\n",
        "        if matching_row.empty:\n",
        "            continue\n",
        "\n",
        "        # Iterate over metals and fill their values from results_data_ICP_op\n",
        "        for metal, cas in metals_dic.items():\n",
        "            try:\n",
        "                # Find the column in results_data_ICP_op that matches the CAS\n",
        "                cas_column_index = results_data_ICP_op.iloc[0, :].tolist().index(cas)  # Exact CAS match in the first row\n",
        "                value_to_fill = matching_row.iloc[0, cas_column_index]  # Extract the correct value\n",
        "                df_sorted.at[index, metal] = value_to_fill\n",
        "            except ValueError:\n",
        "                # CAS not found in the first row\n",
        "                continue\n",
        "\n",
        "if Add_exceptations == True:\n",
        "    # Ensure CAS numbers in both dataframes are properly formatted\n",
        "    results_data_VOC_op[0] = results_data_VOC_op[0].astype(str).str.strip()\n",
        "    threshold_data['CAS No.'] = threshold_data['CAS No.'].astype(str).str.strip()\n",
        "\n",
        "    # Initialize a new row with NaN values\n",
        "    new_row = [None] * results_data_VOC_op.shape[1]\n",
        "\n",
        "    # Iterate through each CAS number in the first row of results_data_VOC_op\n",
        "    for col_index, cas_number in enumerate(results_data_VOC_op.iloc[0, :]):\n",
        "        if pd.isna(cas_number) or cas_number == 'nan':\n",
        "            continue\n",
        "\n",
        "        # Find the matching row in threshold_data\n",
        "        matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "        if not matching_row.empty:\n",
        "            # Get the value from the \"VSL\" column\n",
        "            vsl_value = matching_row.iloc[0]['VSL']\n",
        "            new_row[col_index] = vsl_value\n",
        "\n",
        "    # Append the new row to results_data_VOC_op\n",
        "    results_data_VOC_op.loc[len(results_data_VOC_op)] = new_row\n",
        "\n",
        "    ## Adding the values\n",
        "    # Iterate through each column in results_data_VOC_op\n",
        "    for col_index in range(results_data_VOC_op.shape[1]):\n",
        "        cas_number = results_data_VOC_op.iloc[0, col_index]  # Get the CAS number from row 0\n",
        "        if pd.isna(cas_number) or cas_number == 'nan':\n",
        "            continue\n",
        "\n",
        "        # Check if the CAS number is already in df_sorted\n",
        "        if cas_number in df_sorted.iloc[1, :].values:\n",
        "            continue  # Skip if the CAS number is already present\n",
        "\n",
        "        # Get the column values excluding the first row (CAS number)\n",
        "        column_values = results_data_VOC_op.iloc[1:, col_index]\n",
        "\n",
        "        # Skip columns where all values are NaN\n",
        "        if column_values.isna().all():\n",
        "            continue\n",
        "\n",
        "        # Filter column values to keep only numeric ones\n",
        "        numeric_values = pd.to_numeric(column_values, errors='coerce')\n",
        "\n",
        "        # Get the VSL value (assuming it's in the last row of the column)\n",
        "        vsl_value = results_data_VOC_op.iloc[-1, col_index]\n",
        "        try:\n",
        "            vsl_value = float(vsl_value)  # Ensure VSL value is a float\n",
        "        except ValueError:\n",
        "            continue  # Skip columns where VSL is not a valid number\n",
        "\n",
        "        # **Check for numeric values exceeding VSL**\n",
        "        if (numeric_values > vsl_value).any():\n",
        "            matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "            header_name = matching_row.iloc[0, -1] if not matching_row.empty else f\"Column_{cas_number}\"\n",
        "\n",
        "            # Add new column to df_sorted\n",
        "            df_sorted[header_name] = None\n",
        "            df_sorted.at[1, header_name] = cas_number\n",
        "\n",
        "            # Fill values where 'full name' matches\n",
        "            for index, row in df_sorted.iterrows():\n",
        "                if index <= 1:\n",
        "                    continue\n",
        "                name = row['full name']\n",
        "                if pd.isna(name):\n",
        "                    continue\n",
        "\n",
        "                matching_row = results_data_VOC_op[\n",
        "                    results_data_VOC_op[0].apply(lambda x: x == name if pd.notna(x) else False)\n",
        "                ]\n",
        "                if matching_row.empty:\n",
        "                    continue\n",
        "\n",
        "                value_to_fill = matching_row.iloc[0, col_index]\n",
        "                df_sorted.at[index, header_name] = value_to_fill\n",
        "\n",
        "        # **Check for \"<...\" values where the extracted number is greater than VSL**\n",
        "        for cell_value in column_values:\n",
        "            if isinstance(cell_value, str) and cell_value.startswith(\"<\"):\n",
        "                match = re.search(r'<\\s*(\\d+(\\.\\d+)?)', cell_value)\n",
        "                if match:\n",
        "                    try:\n",
        "                        extracted_value = float(match.group(1))  # Extracted number from \"<...\"\n",
        "\n",
        "                        if extracted_value > vsl_value:  # Compare with VSL\n",
        "                            # Add column if condition is met\n",
        "                            matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "                            header_name = (\n",
        "                                matching_row.iloc[0, -1]\n",
        "                                if not matching_row.empty\n",
        "                                else f\"Column_{cas_number}_BDL\"\n",
        "                            )\n",
        "\n",
        "                            # Ensure column exists in df_sorted\n",
        "                            if header_name not in df_sorted.columns:\n",
        "                                df_sorted[header_name] = None\n",
        "                                df_sorted.at[1, header_name] = cas_number\n",
        "\n",
        "                            # Fill values where 'full name' matches\n",
        "                            for index, row in df_sorted.iterrows():\n",
        "                                if index <= 1:\n",
        "                                    continue\n",
        "                                name = row['full name']\n",
        "                                if pd.isna(name):\n",
        "                                    continue\n",
        "\n",
        "                                matching_row = results_data_VOC_op[\n",
        "                                    results_data_VOC_op[0].apply(lambda x: x == name if pd.notna(x) else False)\n",
        "                                ]\n",
        "                                if matching_row.empty:\n",
        "                                    continue\n",
        "\n",
        "                                value_to_fill = matching_row.iloc[0, col_index]\n",
        "                                df_sorted.at[index, header_name] = value_to_fill\n",
        "\n",
        "                    except ValueError:\n",
        "                        pass  # Skip invalid values\n",
        "\n",
        "if Add_specifics:\n",
        "    Add_specifics_list = [item.strip() for item in Add_specifics.split(',')]\n",
        "    for item in Add_specifics_list:\n",
        "        if item not in results_data_VOC_op.iloc[0].values:\n",
        "            display(Javascript('alert(\"⚠️ one or more of the specific CAS numbers you insert is not in lab results ⚠️\")'))\n",
        "            print(f'{item} is not in lab results')\n",
        "        else:\n",
        "            # Find the column index of the item in results_data_VOC_op\n",
        "            try:\n",
        "                col_index = results_data_VOC_op.iloc[0, :].tolist().index(item)\n",
        "            except ValueError:\n",
        "                print(f'{item} not found in results_data_VOC_op header.')\n",
        "                continue\n",
        "\n",
        "            # Check if the CAS number (item) is already in df_sorted\n",
        "            if item in df_sorted.iloc[1, :].values:\n",
        "                continue  # Skip if the CAS number is already present\n",
        "\n",
        "            # Find the row in results_data_VOC where the CAS number matches\n",
        "            matching_row = threshold_data[\n",
        "                threshold_data['CAS No.'] == item  # Match CAS number in column 1\n",
        "            ]\n",
        "\n",
        "            # If a match is found, get the header from column 0 in the matching row\n",
        "            if not matching_row.empty:\n",
        "                header_name = matching_row.iloc[0, -1]\n",
        "            else:\n",
        "                header_name = f\"Column_{item}\"  # Fallback to a generic name\n",
        "\n",
        "            # Add a new column to df_sorted with the header_name\n",
        "            df_sorted[header_name] = None\n",
        "            df_sorted.at[1, header_name] = item\n",
        "\n",
        "            # Add matching values to df_sorted for the rows where 'full name' matches\n",
        "            for index, row in df_sorted.iterrows():\n",
        "                if index <= 1:  # Skip the header and CAS rows\n",
        "                    continue\n",
        "                name = row['full name']\n",
        "                if pd.isna(name):\n",
        "                    continue\n",
        "\n",
        "                # Find the matching row in results_data_VOC_op\n",
        "                matching_row = results_data_VOC_op[\n",
        "                    results_data_VOC_op[0].apply(lambda x: x == name if pd.notna(x) else False)\n",
        "                ]\n",
        "                if matching_row.empty:\n",
        "                    continue\n",
        "\n",
        "                # Extract the value from results_data_VOC_op for the current column\n",
        "                value_to_fill = matching_row.iloc[0, col_index]\n",
        "                df_sorted.at[index, header_name] = value_to_fill\n",
        "\n",
        "\n",
        "df_sorted.iloc[0, 6:] = 'mg/kg'\n",
        "\n",
        "# Iterate through each column in df_sorted starting from column 6 onward\n",
        "for col_index in range(6, len(df_sorted.columns)):\n",
        "    cas_number = df_sorted.iat[1, col_index]  # Get the CAS number from row 1\n",
        "\n",
        "    if pd.isna(cas_number) or cas_number == '':  # Skip if CAS number is empty or NaN\n",
        "        continue\n",
        "\n",
        "    # Find the matching row in threshold_data based on CAS No.\n",
        "    matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "\n",
        "    if not matching_row.empty:\n",
        "        # Extract the VSL value from the matching row\n",
        "        vsl_value = matching_row.iloc[0]['VSL']\n",
        "        # Populate the cell in row 2 of df_sorted for the current column\n",
        "        df_sorted.iat[2, col_index] = vsl_value\n",
        "\n",
        "        # Extract the value from the column with the name Tier1_type\n",
        "        if Tier1_type in matching_row.columns:\n",
        "            tier1_value = matching_row.iloc[0][Tier1_type]\n",
        "            # Populate the cell in row 3 of df_sorted for the current column\n",
        "            df_sorted.iat[3, col_index] = tier1_value\n",
        "\n",
        "df_sorted = df_sorted.drop(columns='full name')\n",
        "\n",
        "########סידור לטבלת אקסל להורדה########\n",
        "\n",
        "# Fill empty values from row 5 and column 6 onward with \"-\"\n",
        "for row_idx in range(4, len(df_sorted)):\n",
        "    for col_idx in range(5, len(df_sorted.columns)):\n",
        "        val = df_sorted.iat[row_idx, col_idx]\n",
        "        if pd.isna(val) or val == '':\n",
        "            df_sorted.iat[row_idx, col_idx] = \"-\"\n",
        "\n",
        "# Fill empty values in rows 3 and 4 (index 2 and 3) and column 6 onward with \"--\"\n",
        "for row_idx in [2, 3]:\n",
        "    for col_idx in range(5, len(df_sorted.columns)):\n",
        "        val = df_sorted.iat[row_idx, col_idx]\n",
        "        if pd.isna(val) or val == '':\n",
        "            df_sorted.iat[row_idx, col_idx] = \"--\"\n",
        "\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Border, Side, Alignment, PatternFill\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Create an Excel workbook and add a worksheet\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "\n",
        "# Set the sheet to right-to-left\n",
        "ws.sheet_view.rightToLeft = True\n",
        "\n",
        "thin_border = Border(\n",
        "    left=Side(style='thin'),\n",
        "    right=Side(style='thin'),\n",
        "    top=Side(style='thin'),\n",
        "    bottom=Side(style='thin')\n",
        ")\n",
        "\n",
        "alignment_center_wrap = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
        "\n",
        "# Define colors for conditional formatting\n",
        "yellow_fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
        "custom_fill = PatternFill(start_color=\"F7C7AC\", end_color=\"F7C7AC\", fill_type=\"solid\")\n",
        "gray_fill = PatternFill(start_color=\"D3D3D3\", end_color=\"D3D3D3\", fill_type=\"solid\")\n",
        "\n",
        "# Step 1: Add the merged cells for the headers\n",
        "ws.merge_cells(start_row=1, start_column=1, end_row=5, end_column=1)\n",
        "cell = ws.cell(row=1, column=1)\n",
        "cell.value = \"שם קידוח\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "for i in range(2, 6):\n",
        "  cell = ws.cell(row=i, column=1)\n",
        "  cell.border = Border(\n",
        "      left=Side(style='thin'),\n",
        "      right=Side(style='thin'))\n",
        "\n",
        "for i in range(2, 6):  # Iterate over columns (2 to 5 inclusive)\n",
        "    for j in range(1, len(df_sorted)+2):  # Iterate over rows (1 to len(df_sorted) - 1)\n",
        "        cell = ws.cell(row=j, column=i)\n",
        "        cell.border = Border(bottom=Side(style='thin'))\n",
        "\n",
        "ws.merge_cells(start_row=1, start_column=2, end_row=5, end_column=2)\n",
        "cell = ws.cell(row=1, column=2)\n",
        "cell.value = \"תאריך\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=3)\n",
        "cell.value = \"\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=3)\n",
        "cell.value = \"יחידות\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=4)\n",
        "cell.value = \"עומק\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=4)\n",
        "cell.value = \"m\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=5)\n",
        "cell.value = \"PID\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=5)\n",
        "cell.value = \"ppm\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=3, start_column=3, end_row=3, end_column=5)\n",
        "cell = ws.cell(row=3, column=3)\n",
        "cell.value = \"CAS\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=4, start_column=3, end_row=4, end_column=5)\n",
        "cell = ws.cell(row=4, column=3)\n",
        "cell.value = \"VSL\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=5, start_column=3, end_row=5, end_column=5)\n",
        "cell = ws.cell(row=5, column=3)\n",
        "cell.value = \"TIER 1\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "# Helper function to detect if a string contains English letters\n",
        "def contains_english(value):\n",
        "    return any(char.isascii() and char.isalpha() for char in str(value))\n",
        "\n",
        "# Step 2: Add the 'name' column to the first column in the Excel file starting from row 4 of df_sorted\n",
        "for row_idx, value in enumerate(df_sorted[\"name\"][4:], start=6):\n",
        "    cell = ws.cell(row=row_idx, column=1)\n",
        "    cell.value = value\n",
        "    cell.font = Font(\n",
        "        name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "        size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "    )\n",
        "    cell.alignment = alignment_center_wrap\n",
        "    cell.border = thin_border\n",
        "\n",
        "# Merge cells in column 1 with the same value\n",
        "current_start = 6\n",
        "previous_value = None\n",
        "for row_idx, value in enumerate(df_sorted[\"name\"][4:], start=6):\n",
        "    if value == previous_value:\n",
        "        continue\n",
        "    if previous_value is not None:\n",
        "        ws.merge_cells(start_row=current_start, start_column=1, end_row=row_idx - 1, end_column=1)\n",
        "    current_start = row_idx\n",
        "    previous_value = value\n",
        "# Merge the last group\n",
        "if previous_value is not None:\n",
        "    ws.merge_cells(start_row=current_start, start_column=1, end_row=len(df_sorted[\"name\"][4:]) + 5, end_column=1)\n",
        "\n",
        "# Step 3: Add the 'depth' column to column 4 in the Excel file starting from row 6\n",
        "for row_idx, value in enumerate(df_sorted[\"depth\"][4:], start=6):\n",
        "    cell = ws.cell(row=row_idx, column=4)\n",
        "    cell.value = value\n",
        "    cell.font = Font(\n",
        "        name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "        size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "    )\n",
        "    cell.alignment = alignment_center_wrap\n",
        "    cell.border = thin_border\n",
        "\n",
        "# Step 4: Add data from column 6 onward starting from row 1 in Excel\n",
        "for col_idx, col_name in enumerate(df_sorted.columns[5:], start=6):  # Iterate through columns 6 onward\n",
        "    # Add column header\n",
        "    header_cell = ws.cell(row=1, column=col_idx)\n",
        "    header_cell.value = col_name\n",
        "    header_cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "    header_cell.alignment = alignment_center_wrap\n",
        "    header_cell.border = thin_border\n",
        "\n",
        "    # Add column data and round float values\n",
        "    for row_idx, value in enumerate(df_sorted[col_name], start=2):  # Start from row 2 in Excel\n",
        "        cell = ws.cell(row=row_idx, column=col_idx)\n",
        "        try:\n",
        "            value_numeric = round(float(value), 4)  # Round to 4 decimal places\n",
        "            cell.value = value_numeric\n",
        "        except (ValueError, TypeError):\n",
        "            cell.value = value  # Non-numeric values remain unchanged\n",
        "\n",
        "        cell.font = Font(\n",
        "            name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "            size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "        )\n",
        "        cell.alignment = alignment_center_wrap\n",
        "        cell.border = thin_border\n",
        "\n",
        "        # Conditional formatting for rows 6 and onward\n",
        "        if row_idx >= 6:\n",
        "            try:\n",
        "                row_4_value = float(ws.cell(row=4, column=col_idx).value)\n",
        "                row_5_value = float(ws.cell(row=5, column=col_idx).value)\n",
        "\n",
        "                if isinstance(cell.value, (float, int)):\n",
        "                    if cell.value > row_4_value:\n",
        "                        cell.fill = yellow_fill\n",
        "                        cell.font = Font(bold=True, name=cell.font.name, size=cell.font.size)\n",
        "                    if cell.value > row_5_value:\n",
        "                        cell.fill = custom_fill\n",
        "                        cell.font = Font(bold=True, name=cell.font.name, size=cell.font.size)\n",
        "\n",
        "                # **Gray Fill Logic for \"<...\" values (Handles both Integers & Floats)**\n",
        "                elif isinstance(cell.value, str) and cell.value.startswith(\"<\"):\n",
        "                    match = re.search(r'<\\s*(\\d+(\\.\\d+)?)', cell.value)  # Extract numeric part\n",
        "\n",
        "                    if match:\n",
        "                        try:\n",
        "                            numeric_value = float(match.group(1))  # Convert extracted value to float\n",
        "                            threshold_value = ws.cell(row=4, column=col_idx).value  # Get row 4 value\n",
        "\n",
        "                            # Ensure threshold value is numeric before comparison\n",
        "                            if isinstance(threshold_value, (int, float)) and numeric_value > threshold_value:\n",
        "                                cell.fill = gray_fill\n",
        "                                cell.font = Font(bold=True, name=cell.font.name, size=cell.font.size)\n",
        "                        except ValueError:\n",
        "                            pass  # Skip if conversion fails\n",
        "\n",
        "            except (ValueError, TypeError):\n",
        "                pass  # Skip cells that cannot be compared numerically\n",
        "\n",
        "\n",
        "\n",
        "# Merge columns 2 and 3 for each row from row 6 onward\n",
        "for row_idx in range(6, len(df_sorted[\"name\"]) + 6):\n",
        "    ws.merge_cells(start_row=row_idx, start_column=2, end_row=row_idx, end_column=3)\n",
        "\n",
        "# Define the medium border\n",
        "medium_side = Side(style=\"medium\")\n",
        "\n",
        "# Define the range for the table\n",
        "start_row = 1  # Start from the first row\n",
        "end_row = len(df_sorted)+1  # Dynamically determine the last row based on the data\n",
        "start_col = 1  # Start from the first column\n",
        "end_col = ws.max_column  # Dynamically determine the last column based on the data\n",
        "\n",
        "# Apply medium border to the outer edges of the table\n",
        "for row in range(start_row, end_row + 1):\n",
        "    for col in range(start_col, end_col + 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "\n",
        "        # Apply borders to the outer edges only\n",
        "        if row == start_row:  # Top edge\n",
        "            cell.border = Border(top=medium_side, left=cell.border.left, right=cell.border.right, bottom=cell.border.bottom)\n",
        "        if row == end_row:  # Bottom edge\n",
        "            cell.border = Border(bottom=medium_side, left=cell.border.left, right=cell.border.right, top=cell.border.top)\n",
        "        if col == start_col:  # Left edge\n",
        "            cell.border = Border(left=medium_side, top=cell.border.top, bottom=cell.border.bottom, right=cell.border.right)\n",
        "        if col == end_col:  # Right edge\n",
        "            cell.border = Border(right=medium_side, top=cell.border.top, bottom=cell.border.bottom, left=cell.border.left)\n",
        "\n",
        "# Define the range for the first 5 rows\n",
        "start_row = 1\n",
        "end_row = 5\n",
        "start_col = 1\n",
        "end_col = ws.max_column  # Dynamically determine the last column based on the data\n",
        "\n",
        "# Apply thick border to the outer edges\n",
        "for row in range(start_row, end_row + 1):\n",
        "    for col in range(start_col, end_col + 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "\n",
        "        # Apply borders to the outer edges only\n",
        "        if row == start_row:  # Top edge\n",
        "            cell.border = Border(top=medium_side, left=cell.border.left, right=cell.border.right, bottom=cell.border.bottom)\n",
        "        if row == end_row:  # Bottom edge\n",
        "            cell.border = Border(bottom=medium_side, left=cell.border.left, right=cell.border.right, top=cell.border.top)\n",
        "        if col == start_col:  # Left edge\n",
        "            cell.border = Border(left=medium_side, top=cell.border.top, bottom=cell.border.bottom, right=cell.border.right)\n",
        "        if col == end_col:  # Right edge\n",
        "            cell.border = Border(right=medium_side, top=cell.border.top, bottom=cell.border.bottom, left=cell.border.left)\n",
        "\n",
        "# Determine the last row and column of the table\n",
        "last_row = ws.max_row\n",
        "start_col = 1  # Assuming you want the legend to start from the first column\n",
        "\n",
        "# Add legend for DUP and Split notation\n",
        "legend_dup_split = ws.cell(row=len(df_sorted)+2, column=start_col)\n",
        "legend_dup_split.value = \"*Duplicate | **Split\"\n",
        "legend_dup_split.font = Font(bold=True, name=Font_name_english, size=Font_size_english)\n",
        "legend_dup_split.alignment = alignment_center_wrap\n",
        "legend_dup_split.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for - לא בוצעה אנליזה\n",
        "legend_dup_split = ws.cell(row=len(df_sorted)+3, column=start_col)\n",
        "legend_dup_split.value = \" - לא בוצעה אנליזה\"\n",
        "legend_dup_split.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_dup_split.alignment = alignment_center_wrap\n",
        "legend_dup_split.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for ' -- לא קיים ערך סף\n",
        "legend_dup_split = ws.cell(row=len(df_sorted)+4, column=start_col)\n",
        "legend_dup_split.value = \" -- לא קיים ערך סף\"\n",
        "legend_dup_split.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_dup_split.alignment = alignment_center_wrap\n",
        "legend_dup_split.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for \"חורג מערך VSL\"\n",
        "legend_vsl_cell = ws.cell(row=len(df_sorted)+5 , column=start_col)\n",
        "legend_vsl_cell.value = \"חורג מערך VSL\"\n",
        "legend_vsl_cell.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_vsl_cell.alignment = alignment_center_wrap\n",
        "legend_vsl_cell.fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
        "legend_vsl_cell.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for \"חורג מערך TIER 1\"\n",
        "legend_tier1_cell = ws.cell(row=len(df_sorted)+6 , column=start_col)\n",
        "legend_tier1_cell.value = \"חורג מערך TIER 1\"\n",
        "legend_tier1_cell.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_tier1_cell.alignment = alignment_center_wrap\n",
        "legend_tier1_cell.fill = PatternFill(start_color=\"F7C7AC\", end_color=\"F7C7AC\", fill_type=\"solid\")\n",
        "legend_tier1_cell.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for \"ערך גילוי גדול מערך סף\"\n",
        "legend_gray_cell = ws.cell(row=len(df_sorted)+7, column=start_col)\n",
        "legend_gray_cell.value = \"ערך סף גילוי מעבדה גדול מערך סף VSL\"\n",
        "legend_gray_cell.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_gray_cell.alignment = alignment_center_wrap\n",
        "legend_gray_cell.fill = gray_fill\n",
        "legend_gray_cell.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Save the final Excel file with the provided name\n",
        "file_path = f'{file_name}.xlsx'\n",
        "wb.save(file_path)\n",
        "\n",
        "# Download the Excel file\n",
        "files.download(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "-S3t7RXOXGZY",
        "outputId": "e565e837-85ea-49c0-b8d4-ccc4d16f548c",
        "cellView": "form"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2129743695.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  results_data_TPH = results_data_TPH.applymap(remove_parentheses)\n",
            "/tmp/ipython-input-2129743695.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  results_data_VOC = results_data_VOC.applymap(remove_parentheses)\n",
            "/tmp/ipython-input-2129743695.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  results_data_ICP = results_data_ICP.applymap(remove_parentheses)\n",
            "/tmp/ipython-input-2129743695.py:184: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  rows_to_sort['name_prefix'] = rows_to_sort['name'].str.extract(r'([^-\\d]+)', expand=False)\n",
            "/tmp/ipython-input-2129743695.py:185: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  rows_to_sort['name_number'] = rows_to_sort['name'].str.extract(r'(\\d+\\.\\d+|\\d+)', expand=False).astype(float)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7df162f5-baff-41d7-aa4c-70befe01ecad\", \"\\u05ea\\u05d5\\u05e6\\u05d0\\u05d5\\u05ea \\u05d0\\u05e9\\u05db\\u05d5\\u05dc \\u05de\\u05ea\\u05db\\u05d5\\u05ea \\u05e1\\u05d5\\u05e4\\u05d9.xlsx\", 46861)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}