{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***מחברת קוד לעיבוד נתוני תוצאות קייטיאי - סקר קרקע***\n",
        "\n",
        "<bdi>\n",
        "\n",
        "ברירת המחדל של הטבלה הינה הצגת חריגות מערך ה-VSL.\n",
        "\n",
        "ניתן להציג חריגות מערך סף נוסף בהתאם למאפייני האתר.\n",
        "\n",
        "בתא השני נבחר את התרכובות אותם נרצה להציג בטבלת התוצאות. לדוגמה: TPH, MTBE BTEX וכו'.\n",
        "\n",
        "סימון האופציה **Add_exceptations** יציג בטבלה את כל תרכובות ה-VOC בהם יש חריגות מערכי הסף הנבחרים, VSL ו-TIER 1.\n",
        "\n",
        "ב-**Add_specifics** נוסיף מספרי CAS של תרכובות נוספות אותן נרצה להציג על אף שלא אותרו בהן חריגות מערכי הסף.\n",
        "\n",
        "\n",
        "על מנת לעשות זאת, יש לרשום במקום המיועד לכך לפי הפורמט הבא:\n",
        "\n",
        "'cas number, cas number'\n",
        "\n",
        " לדוגמה: '106-93-4, 74-83-9'\n",
        "\n",
        " ניתן הוסיף כמה אנליזות שרוצים, 1,2,3 וכו'.\n",
        "\n",
        " אם תבוקש אנליזה שלא נמצאת בתוצאות מעבדה תקפוץ הודעה, יש ללחוץ אישור ולבדוק איזה אנליזה לא באמת קיימת בקובץ תוצאות שהעלתם\n",
        "\n",
        "**יש לשים לב כי שומרים בשם את קובץ התוצאות מהמעבדה לקובץ אקסל רגיל, מהמעבדה זה מגיע בפורמט מוזר, ורק אז להעלות את קובץ האקסל לכאן, אחרת זה לא יעבוד**\n",
        "</bdi>"
      ],
      "metadata": {
        "id": "gAKKipm41-F1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4dpFJP75WgVI",
        "outputId": "f4193813-570e-4db5-a468-22772a0b1865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "#@title נא לבחור פרמטרים מתאימים  { run: \"auto\" }\n",
        "\n",
        "Tier1_type = \"Tier 1 Residential A >6m\" # @param [\"Tier 1 Residential A 0-6m\",\"Tier 1 Residential A >6m\", \"Tier 1 Residential B\",\"Tier 1 industrial A 0-6m\",\"Tier 1 industrial A >6m\",\"Tier 1 industrial B\"]\n",
        "\n",
        "testsׁ_basic = \"TPH + Metals\" # @param [\"TPH\",\"MTBE BTEX\",\"TPHּּּ + MTBE BTEX\", \"Metals\", \"TPH + Metals\"]\n",
        "\n",
        "Add_exceptations = True # @param {type:\"boolean\"}\n",
        "\n",
        "Add_specifics = None # @param {type:\"raw\"}\n",
        "\n",
        "file_name = 'ניסיון' # @param {type:\"string\"}\n",
        "\n",
        "Font_size_hebrew = 9 # @param {type:\"slider\", min:6, max:24, step:1}\n",
        "\n",
        "Font_name_hebrew = 'David' # @param {type:\"string\"}\n",
        "\n",
        "Font_size_english = 8 # @param {type:\"slider\", min:6, max:24, step:1}\n",
        "\n",
        "Font_name_english = 'Times New Roman' # @param {type:\"string\"}\n",
        "\n",
        "!pip install openpyxl\n",
        "from openpyxl.styles import Font\n",
        "import pandas as pd\n",
        "import re\n",
        "from IPython.display import Javascript\n",
        "import numpy as np\n",
        "\n",
        "if Add_specifics == None:\n",
        "  Add_specifics = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title להעלות קובץ ערכי סף של קרקע בלחיצה על הכפתור למטה\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "filenames = list(uploaded.keys())\n",
        "\n",
        "# Initialize an empty list to store the dataframes\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file and read it based on its extension\n",
        "for filename in filenames:\n",
        "    if filename.endswith('.csv'):\n",
        "        df = pd.read_csv(filename)\n",
        "    elif filename.endswith('.xlsx'):\n",
        "        df = pd.read_excel(filename)\n",
        "    else:\n",
        "        print(f\"Unsupported file format: {filename}\")\n",
        "        continue\n",
        "        # Round all numeric cells to 2 decimal points\n",
        "    #df = df.applymap(lambda x: round(x, 2) if isinstance(x, (int, float)) else x)\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all dataframes into one\n",
        "if dataframes:\n",
        "    threshold_data = pd.concat(dataframes, ignore_index=True)\n",
        "else:\n",
        "    print(\"No valid files uploaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "cellView": "form",
        "id": "c3tohGf-rAhP",
        "outputId": "b24c53a7-0af9-41b9-80dc-486a3fb2a572"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bfcd8951-786d-45f4-a14d-cd8a2db00804\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bfcd8951-786d-45f4-a14d-cd8a2db00804\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving גרסה 7 דצמבר 2024 - soil_threshold.csv to גרסה 7 דצמבר 2024 - soil_threshold (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title להעלות קובץ טבלת תוצאות של קרקע בלחיצה על הכפתור למטה\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# List of uploaded filenames\n",
        "filenames = list(uploaded.keys())\n",
        "\n",
        "# Initialize empty lists to store dataframes for TPH, IPC, and VOC splits\n",
        "dataframes_all = []\n",
        "dataframes_splits = []\n",
        "\n",
        "# Loop through each file and process based on extension\n",
        "for filename in filenames:\n",
        "    if filename.endswith(('.xlsx', '.xls')):\n",
        "        # Read the first sheet\n",
        "        try:\n",
        "            df_all = pd.read_excel(filename, sheet_name='Client SOIL - 1')\n",
        "\n",
        "            dataframes_all.append(df_all)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading first sheet of {filename}: {e}\")\n",
        "\n",
        "        # # Read the second sheet if it exists\n",
        "        # try:\n",
        "        #     df_2 = pd.read_excel(filename, sheet_name='Client SOIL - 2', header=None)\n",
        "        #     # Append the DataFrame to  list\n",
        "        #     dataframes_all.append(df_2)\n",
        "        # except Exception as e:\n",
        "        #     print(f\"No second sheet of {filename}: {e}\")\n",
        "\n",
        "\n",
        "# Concatenate all TPH sheet dataframes into one\n",
        "if dataframes_all:\n",
        "    results_data = pd.concat(dataframes_all, ignore_index=True)\n",
        "else:\n",
        "    print(\"No valid sheets found.\")\n",
        "    results_data = pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert the data to string to avoid errors\n",
        "results_data = results_data.astype(str)\n",
        "\n",
        "split_indices = results_data[results_data.astype(str).apply(lambda x: x.str.contains('Client Sample ID', na=False, case=False)).any(axis=1)].index.tolist()\n",
        "\n",
        "# Split the DataFrame based on detected indices\n",
        "for i in range(len(split_indices)):\n",
        "    start_idx = split_indices[i]\n",
        "    end_idx = split_indices[i + 1] if i + 1 < len(split_indices) else len(results_data)\n",
        "\n",
        "    # Extract the chunk and reset index\n",
        "    results_data = results_data.iloc[start_idx:end_idx].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# # Find all row indices where 'Client Sample ID' appears\n",
        "# split_indices = results_data[results_data.astype(str).apply(lambda x: x.str.contains('Client Sample ID', na=False, case=False)).any(axis=1)].index.tolist()\n",
        "\n",
        "# # Initialize list to store split DataFrames\n",
        "# df_list = []\n",
        "\n",
        "# # Split the DataFrame based on detected indices\n",
        "# for i in range(len(split_indices)):\n",
        "#     start_idx = split_indices[i]\n",
        "#     end_idx = split_indices[i + 1] if i + 1 < len(split_indices) else len(results_data)\n",
        "\n",
        "#     # Extract the chunk and reset index\n",
        "#     df_chunk = results_data.iloc[start_idx:end_idx].reset_index(drop=True)\n",
        "#     df_list.append(df_chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "UdxL50sk7Jcg",
        "outputId": "9022c31d-1ae4-4006-c7f6-0bc512cdba3e",
        "cellView": "form"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-16f7185c-e6ea-4ab6-a664-e6f4517f2a88\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-16f7185c-e6ea-4ab6-a664-e6f4517f2a88\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PR2507576_0_EXCEL_GENERIC.XLS.xlsx to PR2507576_0_EXCEL_GENERIC.XLS.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title יש להריץ את התא ולהוריד את הטבלה המוכנה\n",
        "\n",
        "# Function to remove '(' and ')' from strings\n",
        "def remove_parentheses(cell):\n",
        "    if isinstance(cell, str):  # Only apply to strings\n",
        "        return cell.replace('(', '').replace(')', '')\n",
        "    return cell\n",
        "\n",
        "# Apply to the entire dataframe for results_data\n",
        "results_data = results_data.applymap(remove_parentheses)\n",
        "\n",
        "\n",
        "# Search for the row containing 'Client Sample ID'\n",
        "row_index = results_data[results_data.astype(str).apply(lambda x: x.str.contains('Client Sample ID', na=False, case=False)).any(axis=1)].index\n",
        "\n",
        "# Initialize an empty list to store the extracted drill names\n",
        "drills_list = []\n",
        "\n",
        "if not row_index.empty:\n",
        "    # Get the first occurrence of the row containing 'Client Sample ID'\n",
        "    target_row_index = row_index[0]\n",
        "\n",
        "    # Extract all non-null values from the row, starting from the 'Client Sample ID' column\n",
        "    row_values = results_data.iloc[target_row_index].dropna().tolist()\n",
        "\n",
        "    # Store extracted values into drills_list, excluding 'Client Sample ID'\n",
        "    drills_list.extend(row_values)\n",
        "    if \"Client Sample ID\" in drills_list:\n",
        "        drills_list.remove(\"Client Sample ID\")\n",
        "\n",
        "# Extract non-null and non-string 'nan' values\n",
        "drills_list = [val for val in drills_list if str(val).lower() != 'nan']\n",
        "\n",
        "\n",
        "# Initialize metals_list\n",
        "metals_list = []\n",
        "\n",
        "first_column = results_data.iloc[:, 0].fillna(\"\")\n",
        "\n",
        "# Search for \"Extractable Metals / Major Cations\" in the first column\n",
        "match_index = first_column[first_column.str.strip() == \"Extractable Metals / Major Cations\"].index\n",
        "\n",
        "# Ensure at least one occurrence is found\n",
        "if not match_index.empty:\n",
        "    start_index = match_index[0]  # Take the first occurrence\n",
        "\n",
        "    # Collect all values below it until a NaN is encountered\n",
        "    for i in range(start_index + 1, len(results_data)):\n",
        "        cell_value = results_data.iloc[i, 0]\n",
        "        if pd.isna(cell_value) or cell_value == \"\":  # Stop at NaN or empty cell\n",
        "            break\n",
        "        metals_list.append(cell_value)\n",
        "\n",
        "\n",
        "# Define the structure of the DataFrame\n",
        "columns = ['name', 'date', 'Unnamed: 2', 'depth', ' PID','full name']\n",
        "data = [\n",
        "    ['units', None, None, 'm', 'ppm', None],\n",
        "    ['Cas', None, None, None, None, None],\n",
        "    ['VSL', None, None, None, None, None],\n",
        "    ['TIER 1', None, None, None, None, None]\n",
        "]\n",
        "\n",
        "# # Normalize 'dup', 'duplicate', or 'Duplicate' to 'DUP'\n",
        "# normalized_drills_list = []\n",
        "# for item in drills_list:\n",
        "#     # Replace 'dup', 'duplicate', or 'Duplicate' with 'DUP' (case-insensitive)\n",
        "#     normalized_item = re.sub(r'\\b(?:dup|duplicate|Duplicate)\\b', 'DUP', item, flags=re.IGNORECASE)\n",
        "#     normalized_drills_list.append(normalized_item)\n",
        "\n",
        "# # Remove duplicates by converting to a set, then back to a list to maintain unique items\n",
        "# normalized_drills_list = list(set(normalized_drills_list))\n",
        "\n",
        "# # Sort the list if you want to keep it in a specific order\n",
        "# normalized_drills_list.sort()\n",
        "# drills_list = normalized_drills_list\n",
        "\n",
        "# Create the DataFrame\n",
        "df_template = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# # Process each item in drills_list\n",
        "# df_template['full name'] = None\n",
        "# for item in drills_list:\n",
        "#     # Find the first instance of a number with a dot and numbers on both sides\n",
        "#     match = re.search(r'(\\d+\\.\\d+)', item)\n",
        "#     if match:\n",
        "#         # Extract the depth (value) and the name (key)\n",
        "#         depth = match.group(1)  # The matched number with a dot\n",
        "#         name = item[:match.start()-1].strip()  # Get everything before the depth\n",
        "#         new_row = {'name': name, 'depth': depth, 'full name' : item}  # Create a new row with name and depth\n",
        "#         df_template = pd.concat([df_template, pd.DataFrame([new_row])], ignore_index=True)\n",
        "# # Fill missing columns with None to match the template structure\n",
        "# df_template = df_template.reindex(columns=columns, fill_value=None)\n",
        "\n",
        "# # Create the DataFrame\n",
        "# df_template = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Process each item in drills_list\n",
        "df_template['full name'] = None\n",
        "for item in drills_list:\n",
        "    # Find the first instance of a number with a dot and optional ' DUP' suffix\n",
        "    match = re.search(r'(\\d+\\.\\d+(?: DUP)?)', item)\n",
        "    if match:\n",
        "        # Extract the depth (value) and the name (key)\n",
        "        depth = match.group(1)  # The matched number with a dot and optional ' DUP'\n",
        "        name = item[:match.start()-1].strip()  # Get everything before the depth\n",
        "        new_row = {'name': name, 'depth': depth, 'full name': item}  # Create a new row with name and depth\n",
        "        df_template = pd.concat([df_template, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "# Fill missing columns with None to match the template structure\n",
        "df_template = df_template.reindex(columns=columns, fill_value=None)\n",
        "\n",
        "# Define the starting row for sorting\n",
        "start_row = 4\n",
        "\n",
        "# Extract the header rows (rows before start_row) and the rows to be sorted\n",
        "header_rows = df_template.iloc[:start_row]\n",
        "rows_to_sort = df_template.iloc[start_row:]\n",
        "\n",
        "# Add sorting keys for letter part and number part of the name\n",
        "rows_to_sort['name_letter'] = rows_to_sort['name'].str.extract(r'([A-Za-z]+)', expand=False)\n",
        "rows_to_sort['name_number'] = rows_to_sort['name'].str.extract(r'(\\d+)', expand=False).astype(float)\n",
        "\n",
        "# Sort the rows based on 'name_letter', 'name_number', and 'depth'\n",
        "sorted_rows = rows_to_sort.sort_values(\n",
        "    by=['name_letter', 'name_number', 'depth'],\n",
        "    key=lambda col: pd.to_numeric(col, errors='coerce') if col.name in ['name_number', 'depth'] else col\n",
        ")\n",
        "\n",
        "# Drop the temporary columns used for sorting\n",
        "sorted_rows = sorted_rows.drop(columns=['name_letter', 'name_number'])\n",
        "\n",
        "# Concatenate the header rows with the sorted rows\n",
        "df_sorted = pd.concat([header_rows, sorted_rows], ignore_index=True)\n",
        "\n",
        "####סידור וניקוי נתונים####\n",
        "\n",
        "# Remove rows where all cells are NaN or empty\n",
        "# Replace different types of empty values with actual NaN\n",
        "results_data = results_data.replace([\"\", \" \", \"nan\", \"NaN\", \"None\"], pd.NA)\n",
        "results_data_cleaned = results_data.dropna(how='all')\n",
        "\n",
        "# Transpose the DataFrame\n",
        "results_data_op = results_data_cleaned.T\n",
        "\n",
        "# Reindex the DataFrame\n",
        "results_data_op = results_data_op.reset_index(drop=True)\n",
        "\n",
        "# Drop rows 1-3 by index\n",
        "results_data_op = results_data_op.drop(index=[1, 2, 3]).reset_index(drop=True)\n",
        "\n",
        "# Drop columns 1-4 by index\n",
        "results_data_op = results_data_op.drop(results_data_op.columns[[1,2,3,4]], axis=1)\n",
        "\n",
        "# Identify columns where all values from row 1 onward are NaN\n",
        "cols_to_drop = [col for col in results_data_op.columns if results_data_op.loc[1:, col].isna().all()]\n",
        "\n",
        "# Drop those columns\n",
        "results_data_op = results_data_op.drop(columns=cols_to_drop)\n",
        "\n",
        "# Rename columns numerically\n",
        "results_data_op.columns = range(results_data_op.shape[1])\n",
        "\n",
        "# Set the value of row 0, column 0 to 'Analyte'\n",
        "results_data_op.iat[0, 0] = 'Analyte'\n",
        "\n",
        "# Set row 0 as the new headers\n",
        "results_data_op.columns = results_data_op.iloc[0]  # Assign first row as column names\n",
        "results_data_op = results_data_op[1:].reset_index(drop=True)  # Remove the first row from data\n",
        "\n",
        "def merge_duplicate_columns(df):\n",
        "    new_df = pd.DataFrame()  # Create an empty DataFrame to store merged columns\n",
        "\n",
        "    for col in df.columns.unique():  # Iterate over unique column names\n",
        "        duplicate_cols = df.loc[:, df.columns == col]  # Get all columns with the same name\n",
        "\n",
        "        if duplicate_cols.shape[1] > 1:  # If there are duplicates\n",
        "            merged_col = duplicate_cols.bfill(axis=1).iloc[:, 0]  # Backfill missing values and keep first non-null\n",
        "            new_df[col] = merged_col  # Store merged column\n",
        "        else:\n",
        "            new_df[col] = df[col]  # Keep single columns as they are\n",
        "\n",
        "    return new_df\n",
        "\n",
        "# Merge duplicate columns in df_sorted\n",
        "results_data_op = merge_duplicate_columns(results_data_op)\n",
        "\n",
        "########הכנסת נתונים לטמפלייט########\n",
        "if 'TPH' in testsׁ_basic:\n",
        "  df_sorted['TPH DRO'] = None\n",
        "  df_sorted['TPH ORO'] = None\n",
        "  df_sorted['Total TP'] = None\n",
        "\n",
        "  # Ensure column names are consistent and merge based on \"full name\" and \"Analyte\"\n",
        "  merged_df = df_sorted.merge(\n",
        "      results_data_op.rename(columns={\"Analyte\": \"full name\"}),\n",
        "      on=\"full name\",\n",
        "      how=\"left\"\n",
        "  )\n",
        "\n",
        "  # Function to handle numeric conversion and '<' values properly\n",
        "  def clean_numeric(value):\n",
        "      if isinstance(value, str):\n",
        "          value = value.strip()\n",
        "          if value.startswith(\"<\"):  # If the value starts with '<', extract the number and mark it\n",
        "              try:\n",
        "                  return f\"<{float(value[1:])}\"  # Keep '<' format but extract number\n",
        "              except ValueError:\n",
        "                  return None  # If conversion fails, return None\n",
        "          try:\n",
        "              return float(value)  # Convert to float if possible\n",
        "          except ValueError:\n",
        "              return None  # If conversion fails, return None\n",
        "      return value  # Return original if it's already numeric\n",
        "\n",
        "  # Convert relevant columns to numeric while keeping '<' format\n",
        "  merged_df['C10 - C28 Fraction DRO Numeric'] = merged_df['C10 - C28 Fraction DRO'].apply(clean_numeric)\n",
        "  merged_df['C24 - C40 Fraction ORO Numeric'] = merged_df['C24 - C40 Fraction ORO'].apply(clean_numeric)\n",
        "\n",
        "  # Function to calculate 'Total TP' based on logic\n",
        "  def calculate_total_tph(row):\n",
        "      dro = row['C10 - C28 Fraction DRO Numeric']\n",
        "      oro = row['C24 - C40 Fraction ORO Numeric']\n",
        "\n",
        "      if isinstance(dro, str) and dro.startswith(\"<\") and isinstance(oro, str) and oro.startswith(\"<\"):\n",
        "          # If both are '<' values, sum them as '<total_value'\n",
        "          try:\n",
        "              dro_value = float(dro[1:])\n",
        "              oro_value = float(oro[1:])\n",
        "              return f\"<{dro_value + oro_value}\"\n",
        "          except ValueError:\n",
        "              return '-'  # If extraction fails, return '-'\n",
        "\n",
        "      elif isinstance(dro, str) and dro.startswith(\"<\"):\n",
        "          return oro if isinstance(oro, (int, float)) else dro  # Return numeric value if possible, else keep '<' value\n",
        "\n",
        "      elif isinstance(oro, str) and oro.startswith(\"<\"):\n",
        "          return dro if isinstance(dro, (int, float)) else oro  # Return numeric value if possible, else keep '<' value\n",
        "\n",
        "      elif dro is None and oro is None:\n",
        "          return '-'  # If both are non-numeric, return '-'\n",
        "\n",
        "      elif dro is None:\n",
        "          return oro  # If only DRO is None, return ORO\n",
        "\n",
        "      elif oro is None:\n",
        "          return dro  # If only ORO is None, return DRO\n",
        "\n",
        "      else:\n",
        "          return dro + oro  # If both are numeric, sum them\n",
        "\n",
        "  # Apply the function to calculate 'Total TP'\n",
        "  df_sorted['Total TP'] = merged_df.apply(calculate_total_tph, axis=1)\n",
        "\n",
        "  # Update the relevant columns in df_sorted\n",
        "  df_sorted['TPH DRO'] = merged_df['C10 - C28 Fraction DRO']\n",
        "  df_sorted['TPH ORO'] = merged_df['C24 - C40 Fraction ORO']\n",
        "\n",
        "  # Renaming the column 'Total TP' to 'Total TPH'\n",
        "  df_sorted.rename(columns={'Total TP': 'Total TPH'}, inplace=True)\n",
        "\n",
        "  #Adding CAS number\n",
        "  df_sorted['TPH DRO'][1] = \"C10-C40\"\n",
        "  df_sorted['TPH ORO'][1] = \"C10-C40\"\n",
        "  df_sorted['Total TPH'][1] = \"C10-C40\"\n",
        "\n",
        "\n",
        "cas_dict = {\n",
        "    \"Benzene\": \"71-43-2\",     \"Ethylbenzene\": \"100-41-4\",     \"Sum of xylenes\": \"1330-20-7\",     \"Toluene\": \"108-88-3\",     \"meta- & para-Xylene\": \"108-38-3\",     \"ortho-Xylene\": \"95-47-6\",\n",
        "    \"Benzyl Alcohol\": \"100-51-6\",     \"6-Caprolactam\": \"105-60-2\",     \"Acetophenone\": \"98-86-2\",     \"Isophorone\": \"78-59-1\",     \"4-Chloroaniline\": \"106-47-8\",     \"Aniline\": \"62-53-3\",\n",
        "    \"Benzidine\": \"92-87-5\",     \"Diphenylamine\": \"122-39-4\",     \"1,1`-Biphenyl\": \"92-52-4\",     \"1-Chloronaphthalene\": \"nan\",     \"2-Chloronaphthalene\": \"91-58-7\",     \"1-Methylnaphthalene\": \"90-12-0\",\n",
        "    \"2-Methylnaphthalene\": \"91-57-6\",     \"4-Bromophenyl phenyl ether\": \"nan\",     \"4-Chlorophenyl phenyl ether\": \"nan\",     \"Carbazole\": \"nan\",     \"Dibenzofuran\": \"132-64-9\",     \"1.1-Dichloroethane\": \"75-34-3\",\n",
        "    \"1.1-Dichloroethene\": \"nan\",     \"1.1-Dichloropropene\": \"nan\",     \"1.1.1-Trichloroethane\": \"71-55-6\",     \"1.1.1.2-Tetrachloroethane\": \"630-20-6\",     \"1.1.2-Trichloroethane\": \"79-00-5\",     \"1.1.2.2-Tetrachloroethane\": \"79-34-5\",\n",
        "    \"1.2-Dibromo-3-chloropropane\": \"96-12-8\",     \"1.2-Dibromoethane EDB\": \"106-93-4\",     \"1.2-Dichlorobenzene\": \"95-50-1\",     \"1.2-Dichloroethane\": \"107-06-2\",     \"1.2-Dichloropropane\": \"78-87-5\",     \"1.2.3-Trichlorobenzene\": \"87-61-6\",\n",
        "    \"1.2.3-Trichloropropane\": \"96-18-4\",     \"1.2.4-Trichlorobenzene\": \"87-61-6\",     \"1.3-Dichlorobenzene\": \"nan\",     \"1.3-Dichloropropane\": \"142-28-9\",     \"1.4-Dichlorobenzene\": \"106-46-7\",     \"2-Chlorotoluene\": \"95-49-8\",\n",
        "    \"4-Chlorotoluene\": \"106-43-4\",     \"2.2-Dichloropropane\": \"nan\",     \"Bromobenzene\": \"108-86-1\",     \"Bromochloromethane\": \"74-97-5\",     \"Bromodichloromethane\": \"75-27-4\",     \"Bromoform\": \"75-25-2\",\n",
        "    \"Bromomethane\": \"74-83-9\",     \"Chlorobenzene\": \"108-90-7\",     \"Chloroethane\": \"75-00-3\",     \"Chloroform\": \"67-66-3\",     \"Chloromethane\": \"74-87-3\",     \"Dibromochloromethane\": \"124-48-1\",\n",
        "    \"Dibromomethane\": \"74-95-3\",     \"Dichlorodifluoromethane\": \"75-71-8\",     \"Dichloromethane\": \"nan\",     \"Hexachlorobutadiene\": \"87-68-3\",     \"Tetrachloroethene\": \"nan\",     \"Tetrachloromethane\": \"nan\",\n",
        "    \"Trichloroethene\": \"nan\",     \"Vinyl chloride\": \"75-01-4\",     \"cis-1.3-Dichloropropene\": \"542-75-6\",     \"trans-1.3-Dichloropropene\": \"542-75-6\",     \"cis-1.2-Dichloroethene\": \"nan\",     \"trans-1.2-Dichloroethene\": \"nan\",\n",
        "    \"1.2.4-Trimethylbenzene\": \"95-63-6\",     \"1.3.5-Trimethylbenzene\": \"108-67-8\",     \"1.4-Dioxane\": \"123-91-1\",     \"2-Butanone MEK\": \"78-93-3\",     \"Acetone\": \"67-64-1\",     \"Isopropylbenzene\": \"98-82-8\",\n",
        "    \"Methyl isobutyl ketone\": \"108-10-1\",     \"Methyl tert-Butyl Ether MTBE\": \"1634-04-4\",     \"Styrene\": \"100-42-5\",     \"n-Butylbenzene\": \"104-51-8\",     \"n-Propylbenzene\": \"nan\",     \"p-Isopropyltoluene\": \"nan\",\n",
        "    \"sec-Butylbenzene\": \"135-98-8\",     \"tert-Butyl alcohol\": \"nan\",     \"tert-Butylbenzene\": \"98-06-6\",     \"1.2.3.4-Tetrachlorobenzene\": \"nan\",     \"1.2.3.5- & 1.2.4.5-Tetrachlorobenzene\": \"95-94-3\",     \"2.4-DDD\": \"nan\",\n",
        "    \"4.4`-DDD\": \"72-54-8\",     \"2.4-DDE\": \"nan\",     \"2.4-DDT\": \"50-29-3\",     \"4.4`-DDE\": \"72-55-9\",     \"4.4`-DDT\": \"50-29-3\",     \"Alachlor\": \"15972-60-8\",\n",
        "    \"Aldrin\": \"309-00-2\",     \"Chlordane-cis\": \"12789-03-6\",     \"Chlordane-trans\": \"12789-03-6\",     \"Dieldrin\": \"60-57-1\",     \"Endosulfan sulfate\": \"115-29-7\",     \"Endrin\": \"72-20-8\",\n",
        "    \"Heptachlor\": \"76-44-8\",     \"Heptachloroepoxide-cis\": \"1024-57-3\",     \"Heptachloroepoxide-trans\": \"1024-57-3\",     \"Hexachlorobenzene HCB\": \"118-74-1\",     \"Hexachlorocyclohexane Alpha\": \"319-84-6\",     \"Hexachlorocyclohexane Beta\": \"319-85-7\",\n",
        "    \"Hexachlorocyclohexane Delta\": \"nan\",     \"Hexachlorocyclohexane Epsilon\": \"nan\",     \"Hexachlorocyclohexane Gamma\": \"58-89-9\",     \"Hexachloroethane\": \"67-72-1\",     \"Isodrin\": \"nan\",     \"Methoxychlor\": \"72-43-5\",\n",
        "    \"Mirex\": \"2385-85-5\",     \"Nonachlor-cis\": \"nan\",     \"Nonachlor-trans\": \"nan\",     \"Oxychlordane\": \"nan\",     \"Pentachlorobenzene\": \"608-93-5\",     \"Sum of 3 tetrachlorobenzenes\": \"nan\",\n",
        "    \"Sum of 3 tetrachlorobenzenes M1\": \"nan\",     \"Sum of 4 hexachlorcyclohexanes\": \"nan\",     \"Sum of 4 hexachlorocyclohexanes M1\": \"nan\",     \"Sum of 4 isomers DDT\": \"nan\",     \"Sum of 4 isomers DDT M1\": \"nan\",     \"Sum of 6 isomers DDT\": \"nan\",\n",
        "    \"Sum of 6 isomers DDT M1\": \"nan\",     \"Telodrin\": \"nan\",     \"Trifluralin\": \"1582-09-8\",     \"alpha-Endosulfan\": \"115-29-7\",     \"beta-Endosulfan\": \"115-29-7\",     \"PBB 153\": \"nan\",\n",
        "    \"Dry matter @ 105ֲ°C\": \"nan\",     \"Naphthalene\": \"91-20-3\",     \"C10 - C28 Fraction DRO\": \"Diesel Range Organics\",     \"C24 - C40 Fraction ORO\": \"Oil Range Organics\",     \"Bis2-chloroethoxymethane\": \"111-91-1\",     \"Bis2-chloroethylether\": \"111-44-4\",\n",
        "    \"Bis2-chloroisopropylether all isomers\": \"nan\",     \"2-Chlorophenol\": \"95-57-8\",     \"2.4.5-Trichlorophenol\": \"95-95-4\",     \"2.4.6-Trichlorophenol\": \"88-06-2\",     \"2.4@2.5-Dichlorophenol\": \"120-83-2\",     \"2.6-Dichlorophenol\": \"nan\",\n",
        "    \"Pentachlorophenol\": \"87-86-5\",     \"2,4-Dimethylphenol\": \"95-65-8\",     \"2-Methylphenol\": \"nan\",     \"3- & 4-Methylphenol\": \"nan\",     \"4-Chloro-3-methylphenol\": \"nan\",     \"Phenol\": \"108-95-2\",\n",
        "    \"Aluminium\": \"7429-90-5\",     \"Antimony\": \"7440-36-0\",     \"Arsenic\": \"7440-38-2\",     \"Barium\": \"7440-39-3\",     \"Beryllium\": \"7440-41-7\",     \"Bismuth\": \"nan\",\n",
        "    \"Boron\": \"nan\",     \"Cadmium\": \"7440-43-9\",     \"Calcium\": \"nan\",     \"Chromium\": \"7440-47-3\",     \"Cobalt\": \"7440-48-4\",     \"Copper\": \"7440-50-8\",\n",
        "    \"Iron\": \"7439-89-6\", \"Lead\" : \"7439-92-1\" ,   \"Lithium\": \"7439-93-2\",     \"Magnesium\": \"nan\",     \"Manganese\": \"7439-96-5\",     \"Mercury\": \"7439-97-6\",     \"Molybdenum\": \"7439-98-7\",\n",
        "    \"Nickel\": \"7440-02-0\",     \"Phosphorus\": \"7723-14-0\",     \"Potassium\": \"nan\",     \"Selenium\": \"7782-49-2\",     \"Silicon\": \"nan\",     \"Silver\": \"7440-22-4\",\n",
        "    \"Sodium\": \"nan\",     \"Strontium\": \"7440-24-6\",     \"Sulphur\": \"nan\",     \"Tellurium\": \"nan\",     \"Thallium\": \"7440-28-0\",     \"Tin\": \"7440-31-5\",\n",
        "    \"Titanium\": \"7550-45-0\",     \"Vanadium\": \"7440-62-2\",     \"Zinc\": \"7440-66-6\",     \"Zirconium\": \"7440-67-7\",     \"Hexachlorocyclopentadiene\": \"77-47-4\",     \"2,4-Dinitrophenol\": \"51-28-5\",\n",
        "    \"2,4-Dinitrotoluene\": \"121-14-2\",     \"2-Nitroaniline\": \"88-74-4\",     \"2-Nitrophenol\": \"nan\",     \"2.6-Dinitrotoluene\": \"606-20-2\",     \"3-Nitroaniline\": \"nan\",     \"4,6-Dinitro-2-methylphenol\": \"nan\",\n",
        "    \"4-Nitroaniline\": \"100-01-6\",     \"4-Nitrophenol\": \"nan\",     \"Nitrobenzene\": \"98-95-3\",     \"N-Nitrosodi-n-propylamine\": \"621-64-7\",     \"Total SVOC IL M4\": \"nan\",     \"Dinoseb\": \"88-85-7\",\n",
        "    \"Bis2-ethylhexylphthalate\": \"117-81-7\",     \"Butyl benzyl phthalate\": \"85-68-7\",     \"Di-n-butyl phthalate\": \"84-74-2\",     \"Di-n-octyl phthalate\": \"117-84-0\",     \"Diethyl phthalate\": \"84-66-2\",     \"Dimethyl phthalate\": \"nan\",\n",
        "    \"pH H2O\": \"nan\",     \"Acenaphthene\": \"83-32-9\",     \"Acenaphthylene\": \"nan\",     \"Anthracene\": \"120-12-7\",     \"Benzaanthracene\": \"56-55-3\",     \"Benzoapyrene\": \"50-32-8\",\n",
        "    \"Benzobfluoranthene\": \"205-99-2\",     \"Benzog.h.iperylene\": \"191-24-2\",     \"Benzokfluoranthene\": \"207-08-9\",     \"Chrysene\": \"218-01-9\",     \"Dibenza.hanthracene\": \"53-70-3\",     \"Fluoranthene\": \"206-44-0\",\n",
        "    \"Fluorene\": \"86-73-7\",     \"Indeno1.2.3.cdpyrene\": \"193-39-5\",     \"Phenanthrene\": \"nan\",     \"Pyrene\": \"129-00-0\",\n",
        "}\n",
        "\n",
        "# Function to apply renaming rules robustly\n",
        "def rename_chemical(name):\n",
        "    if pd.isna(name):  # Handle NaN values safely\n",
        "        return name\n",
        "\n",
        "    # Rule 1: Move numerical prefixes before the chemical name and replace commas between numbers with dots\n",
        "    pattern1 = r\"(\\w+), ([\\d,.-]+)-\"  # Matches \"Name, Number(s)-\" pattern\n",
        "    match = re.search(pattern1, name)\n",
        "    if match:\n",
        "        base_name, prefix = match.groups()\n",
        "        formatted_prefix = prefix.replace(\",\", \".\")  # Convert ',' between numbers to '.'\n",
        "        name = f\"{formatted_prefix}-{base_name}\"  # Move prefix before the name\n",
        "\n",
        "    # Rule 2: Handling \"p-\", \"o-\", \"m-\" prefixes (e.g., \"Aminophenol, p-\" → \"4-Aminophenol\")\n",
        "    positional_mapping = {\n",
        "        \"p-\": \"4-\",\n",
        "        \"o-\": \"2-\",\n",
        "        \"m-\": \"3-\"\n",
        "    }\n",
        "\n",
        "    pattern2 = r\"(\\w+), (p-|o-|m-)$\"\n",
        "    match = re.search(pattern2, name)\n",
        "    if match:\n",
        "        base_name, prefix = match.groups()\n",
        "        name = f\"{positional_mapping[prefix]}{base_name}\"  # Replace with correct numeric prefix\n",
        "\n",
        "    return name\n",
        "\n",
        "\n",
        "\n",
        "# Apply renaming rules to the \"Chemical\" column\n",
        "threshold_data[\"Chemical\"] = threshold_data[\"Chemical\"].apply(rename_chemical)\n",
        "\n",
        "import numpy as np\n",
        "from difflib import get_close_matches\n",
        "\n",
        "# Create a new row with CAS numbers\n",
        "cas_row = [\"CAS No.\"]  # First cell as a label\n",
        "\n",
        "# Extract \"Chemical\" and \"CAS No.\" from threshold_data for matching\n",
        "chemicals_threshold = threshold_data[['Chemical', 'CAS No.']].dropna()\n",
        "\n",
        "# Iterate through each column header in results_data_op (excluding the first column)\n",
        "for chem in results_data_op.columns[1:]:\n",
        "    cas_value = cas_dict.get(chem, np.nan)  # Lookup CAS number in dictionary, if not found, set NaN\n",
        "\n",
        "    # If CAS is NaN, try finding the best match from threshold_data['Chemical']\n",
        "    if pd.isna(cas_value):\n",
        "        best_match = get_close_matches(chem, chemicals_threshold['Chemical'], n=1, cutoff=0.8)\n",
        "        if best_match:\n",
        "            matched_row = chemicals_threshold[chemicals_threshold['Chemical'] == best_match[0]]\n",
        "            cas_value = matched_row['CAS No.'].values[0] if not matched_row.empty else np.nan\n",
        "\n",
        "    cas_row.append(cas_value)\n",
        "\n",
        "# Insert the CAS No. row at the top of results_data_op\n",
        "cas_no_df = pd.DataFrame([cas_row], columns=results_data_op.columns)\n",
        "results_data_op = pd.concat([cas_no_df, results_data_op], ignore_index=True)\n",
        "\n",
        "# Initialize metals dictionary\n",
        "metals_dict = {}\n",
        "\n",
        "# Iterate over each metal in metals_list\n",
        "for metal in metals_list:\n",
        "    if metal in results_data_op.columns:  # Check if metal exists in headers\n",
        "        cas_value = results_data_op.loc[0, metal]  # Get CAS number from row 0\n",
        "        metals_dict[metal] = cas_value  # Add to dictionary\n",
        "\n",
        "#Remove the headers names\n",
        "results_data_op.columns = range(len(results_data_op.columns))\n",
        "\n",
        "if 'MTBE' in testsׁ_basic:\n",
        "  df_sorted['MTBE'] = None\n",
        "  df_sorted['Benzene'] = None\n",
        "  df_sorted['Toluene'] = None\n",
        "  df_sorted['Ethylbenzene'] = None\n",
        "  df_sorted['Xylene'] = None\n",
        "\n",
        "  #Adding CAS number\n",
        "  df_sorted['MTBE'][1] = '1634-04-4'\n",
        "  df_sorted['Benzene'][1] = '71-43-2'\n",
        "  df_sorted['Toluene'][1] = '108-88-3'\n",
        "  df_sorted['Ethylbenzene'][1] = '100-41-4'\n",
        "  df_sorted['Xylene'][1] = '95-47-6'\n",
        "\n",
        "  # Ensure matching names are correctly formatted\n",
        "  df_sorted['full name'] = df_sorted['full name'].astype(str).str.strip()\n",
        "  results_data_op[0] = results_data_op[0].astype(str).str.strip()\n",
        "\n",
        "  # Extract CAS mappings for the required columns\n",
        "  cas_map = df_sorted.loc[1, [\"MTBE\", \"Benzene\", \"Toluene\", \"Ethylbenzene\", \"Xylene\"]].to_dict()\n",
        "\n",
        "  # Iterate over each row in df_sorted and fill in the values\n",
        "  for index, row in df_sorted.iterrows():\n",
        "      if index <= 1:  # Skip non-data rows\n",
        "          continue\n",
        "      name = row['full name']\n",
        "      if pd.isna(name):\n",
        "          continue\n",
        "\n",
        "      # Find the matching name in results_data_op (reversed logic)\n",
        "      matching_row = results_data_op[results_data_op[0].apply(lambda x: x == name if pd.notna(x) else False)]\n",
        "      if matching_row.empty:\n",
        "          continue\n",
        "\n",
        "      for column, cas in cas_map.items():\n",
        "          if pd.isna(cas):\n",
        "              continue\n",
        "\n",
        "          # Find the column in results_data_VOC_op that matches the CAS\n",
        "          try:\n",
        "              cas_column_index = results_data_op.iloc[0, :].tolist().index(cas)  # Exact CAS match in the first row\n",
        "              value_to_fill = matching_row.iloc[0, cas_column_index]  # Extract the correct value\n",
        "              df_sorted.at[index, column] = value_to_fill\n",
        "          except ValueError:\n",
        "              # CAS not found in the first row\n",
        "              continue\n",
        "\n",
        "# Adding metals to df_sorted if 'Metals' is in tests_basic\n",
        "if 'Metals' in testsׁ_basic:\n",
        "    # Initialize columns for each metal in df_sorted\n",
        "    for metal, cas in metals_dict.items():\n",
        "        df_sorted[metal] = None\n",
        "\n",
        "    # Add CAS numbers in the second row (index 1) for each metal\n",
        "    for metal, cas in metals_dict.items():\n",
        "        df_sorted.at[1, metal] = cas\n",
        "\n",
        "    # Ensure matching names are correctly formatted\n",
        "    df_sorted['full name'] = df_sorted['full name'].astype(str).str.strip()\n",
        "\n",
        "    # Iterate over each row in df_sorted and fill in the values\n",
        "    for index, row in df_sorted.iterrows():\n",
        "        if index <= 1:  # Skip non-data rows\n",
        "            continue\n",
        "        name = row['full name']\n",
        "        if pd.isna(name):\n",
        "            continue\n",
        "\n",
        "        # Find the matching name in results_data_ICP_op\n",
        "        matching_row = results_data_op[results_data_op[0].apply(lambda x: x == name if pd.notna(x) else False)]\n",
        "        if matching_row.empty:\n",
        "            continue\n",
        "\n",
        "        # Iterate over metals and fill their values from results_data_ICP_op\n",
        "        for metal, cas in metals_dict.items():\n",
        "            try:\n",
        "                # Find the column in results_data_ICP_op that matches the CAS\n",
        "                cas_column_index = results_data_op.iloc[0, :].tolist().index(cas)  # Exact CAS match in the first row\n",
        "                value_to_fill = matching_row.iloc[0, cas_column_index]  # Extract the correct value\n",
        "                df_sorted.at[index, metal] = value_to_fill\n",
        "            except ValueError:\n",
        "                # CAS not found in the first row\n",
        "                continue\n",
        "\n",
        "if Add_exceptations == True:\n",
        "  # Ensure CAS numbers in both dataframes are properly formatted\n",
        "  results_data_op.iloc[0, :] = results_data_op.iloc[0, :].astype(str).str.strip()\n",
        "  threshold_data['CAS No.'] = threshold_data['CAS No.'].astype(str).str.strip()\n",
        "\n",
        "  # Initialize a new row with NaN values\n",
        "  new_row = [None] * results_data_op.shape[1]\n",
        "\n",
        "  # Iterate through each CAS number in the first row of results_data_op\n",
        "  for col_index, cas_number in enumerate(results_data_op.iloc[0, :]):\n",
        "      if pd.isna(cas_number) or cas_number == 'nan':\n",
        "          continue\n",
        "\n",
        "      # Find the matching row in threshold_data\n",
        "      matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "      if not matching_row.empty:\n",
        "          # Get the value from the \"VSL\" column\n",
        "          vsl_value = matching_row.iloc[0]['VSL']\n",
        "          new_row[col_index] = vsl_value\n",
        "\n",
        "  # Append the new row to results_data_op\n",
        "  results_data_op.loc[len(results_data_op)] = new_row\n",
        "\n",
        "  ##adding the values\n",
        "  # Iterate through each column in results_data_op\n",
        "  for col_index in range(results_data_op.shape[1]):\n",
        "      cas_number = results_data_op.iloc[0, col_index]  # Get the CAS number from row 0\n",
        "      if pd.isna(cas_number) or cas_number == 'nan':\n",
        "          continue\n",
        "\n",
        "      # Check if the CAS number is already in df_sorted\n",
        "      if cas_number in df_sorted.iloc[1, :].values:\n",
        "          continue  # Skip if the CAS number is already present\n",
        "\n",
        "      # Get the column values excluding the first row (CAS number)\n",
        "      column_values = results_data_op.iloc[1:, col_index]\n",
        "\n",
        "      # Skip columns where all values are NaN\n",
        "      if column_values.isna().all():\n",
        "          continue\n",
        "\n",
        "      # Filter column values to keep only numeric ones\n",
        "      numeric_values = pd.to_numeric(column_values, errors='coerce')\n",
        "\n",
        "      # Get the VSL value (assuming it's in the last row of the column)\n",
        "      vsl_value = results_data_op.iloc[-1, col_index]\n",
        "      try:\n",
        "          vsl_value = float(vsl_value)  # Ensure VSL value is a float\n",
        "      except ValueError:\n",
        "          continue  # Skip columns where VSL is not a valid number\n",
        "\n",
        "      # Check if any numeric value in the column exceeds the VSL value\n",
        "      if (numeric_values > vsl_value).any():\n",
        "          # Find the row in results_data_VOC where the CAS number matches\n",
        "          matching_row = threshold_data[\n",
        "                threshold_data['CAS No.'] == cas_number  # Match CAS number in column 1\n",
        "            ]\n",
        "\n",
        "          # If a match is found, get the header from column 0 in the matching row\n",
        "          if not matching_row.empty:\n",
        "              header_name = matching_row.iloc[0, -1]\n",
        "          else:\n",
        "              header_name = f\"Column_{cas_number}\"  # Fallback to a generic name\n",
        "\n",
        "          # Add a new column to df_sorted with the header_name\n",
        "          df_sorted[header_name] = None\n",
        "          df_sorted.at[1, header_name] = cas_number\n",
        "\n",
        "          # Add matching values to df_sorted for the rows where 'full name' matches\n",
        "          for index, row in df_sorted.iterrows():\n",
        "              if index <= 1:  # Skip the header and CAS rows\n",
        "                  continue\n",
        "              name = row['full name']\n",
        "              if pd.isna(name):\n",
        "                  continue\n",
        "\n",
        "              # Find the matching row in results_data_VOC_op\n",
        "              matching_row = results_data_op[\n",
        "                  results_data_op[0].apply(lambda x: x == name if pd.notna(x) else False)\n",
        "              ]\n",
        "              if matching_row.empty:\n",
        "                  continue\n",
        "\n",
        "              # Extract the value from results_data_op for the current column\n",
        "              value_to_fill = matching_row.iloc[0, col_index]\n",
        "              df_sorted.at[index, header_name] = value_to_fill\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if Add_specifics:\n",
        "    Add_specifics_list = [item.strip() for item in Add_specifics.split(',')]\n",
        "    for item in Add_specifics_list:\n",
        "        if item not in results_data_op.iloc[0].values:\n",
        "            display(Javascript('alert(\"⚠️ one or more of the specific CAS numbers you insert is not in lab results ⚠️\")'))\n",
        "            print(f'{item} is not in lab results')\n",
        "        else:\n",
        "            # Find the column index of the item in results_data_op\n",
        "            try:\n",
        "                col_index = results_data_op.iloc[0, :].tolist().index(item)\n",
        "            except ValueError:\n",
        "                print(f'{item} not found in results_data_op header.')\n",
        "                continue\n",
        "\n",
        "            # Check if the CAS number (item) is already in df_sorted\n",
        "            if item in df_sorted.iloc[1, :].values:\n",
        "                continue  # Skip if the CAS number is already present\n",
        "\n",
        "            # Find the row in results_data_VOC where the CAS number matches\n",
        "            matching_row = threshold_data[\n",
        "                threshold_data['CAS No.'] == item  # Match CAS number in column 1\n",
        "            ]\n",
        "\n",
        "            # If a match is found, get the header from column 0 in the matching row\n",
        "            if not matching_row.empty:\n",
        "                header_name = matching_row.iloc[0, -1]\n",
        "            else:\n",
        "                header_name = f\"Column_{item}\"  # Fallback to a generic name\n",
        "\n",
        "            # Add a new column to df_sorted with the header_name\n",
        "            df_sorted[header_name] = None\n",
        "            df_sorted.at[1, header_name] = item\n",
        "\n",
        "            # Add matching values to df_sorted for the rows where 'full name' matches\n",
        "            for index, row in df_sorted.iterrows():\n",
        "                if index <= 1:  # Skip the header and CAS rows\n",
        "                    continue\n",
        "                name = row['full name']\n",
        "                if pd.isna(name):\n",
        "                    continue\n",
        "\n",
        "                # Find the matching row in results_data_op\n",
        "                matching_row = results_data_op[\n",
        "                    results_data_op[0].apply(lambda x: x == name if pd.notna(x) else False)\n",
        "                ]\n",
        "                if matching_row.empty:\n",
        "                    continue\n",
        "\n",
        "                # Extract the value from results_data_op for the current column\n",
        "                value_to_fill = matching_row.iloc[0, col_index]\n",
        "                df_sorted.at[index, header_name] = value_to_fill\n",
        "\n",
        "\n",
        "df_sorted.iloc[0, 6:] = 'mg/kg'\n",
        "\n",
        "# Convert all <NA> values to NaN\n",
        "df_sorted = df_sorted.astype(object).where(pd.notna(df_sorted), np.nan)\n",
        "\n",
        "# Iterate through each column in df_sorted starting from column 6 onward\n",
        "for col_index in range(6, len(df_sorted.columns)):\n",
        "    cas_number = df_sorted.iat[1, col_index]  # Get the CAS number from row 1\n",
        "\n",
        "    if pd.isna(cas_number) or cas_number == '':  # Skip if CAS number is empty or NaN\n",
        "        continue\n",
        "\n",
        "    # Find the matching row in threshold_data based on CAS No.\n",
        "    matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "\n",
        "    if not matching_row.empty:\n",
        "        # Extract the VSL value from the matching row\n",
        "        vsl_value = matching_row.iloc[0]['VSL']\n",
        "        # Populate the cell in row 2 of df_sorted for the current column\n",
        "        df_sorted.iat[2, col_index] = vsl_value\n",
        "\n",
        "        # Extract the value from the column with the name Tier1_type\n",
        "        if Tier1_type in matching_row.columns:\n",
        "            tier1_value = matching_row.iloc[0][Tier1_type]\n",
        "            # Populate the cell in row 3 of df_sorted for the current column\n",
        "            df_sorted.iat[3, col_index] = tier1_value\n",
        "\n",
        "df_sorted = df_sorted.drop(columns='full name')\n",
        "\n",
        "########סידור לטבלת אקסל להורדה########\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Border, Side, Alignment, PatternFill\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import pandas as pd\n",
        "\n",
        "# Create an Excel workbook and add a worksheet\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "\n",
        "# Set the sheet to right-to-left\n",
        "ws.sheet_view.rightToLeft = True\n",
        "\n",
        "thin_border = Border(\n",
        "    left=Side(style='thin'),\n",
        "    right=Side(style='thin'),\n",
        "    top=Side(style='thin'),\n",
        "    bottom=Side(style='thin')\n",
        ")\n",
        "\n",
        "alignment_center_wrap = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
        "\n",
        "# Define colors for conditional formatting\n",
        "yellow_fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
        "custom_fill = PatternFill(start_color=\"F7C7AC\", end_color=\"F7C7AC\", fill_type=\"solid\")\n",
        "\n",
        "# Step 1: Add the merged cells for the headers\n",
        "ws.merge_cells(start_row=1, start_column=1, end_row=5, end_column=1)\n",
        "cell = ws.cell(row=1, column=1)\n",
        "cell.value = \"שם קידוח\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "for i in range(2, 6):\n",
        "  cell = ws.cell(row=i, column=1)\n",
        "  cell.border = Border(\n",
        "      left=Side(style='thin'),\n",
        "      right=Side(style='thin'))\n",
        "\n",
        "for i in range(2, 6):  # Iterate over columns (2 to 5 inclusive)\n",
        "    for j in range(1, len(df_sorted)+2):  # Iterate over rows (1 to len(df_sorted) - 1)\n",
        "        cell = ws.cell(row=j, column=i)\n",
        "        cell.border = Border(bottom=Side(style='thin'))\n",
        "\n",
        "ws.merge_cells(start_row=1, start_column=2, end_row=5, end_column=2)\n",
        "cell = ws.cell(row=1, column=2)\n",
        "cell.value = \"תאריך\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=3)\n",
        "cell.value = \"\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=3)\n",
        "cell.value = \"יחידות\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=4)\n",
        "cell.value = \"עומק\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=4)\n",
        "cell.value = \"m\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=5)\n",
        "cell.value = \"PID\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=5)\n",
        "cell.value = \"ppm\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=3, start_column=3, end_row=3, end_column=5)\n",
        "cell = ws.cell(row=3, column=3)\n",
        "cell.value = \"CAS\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=4, start_column=3, end_row=4, end_column=5)\n",
        "cell = ws.cell(row=4, column=3)\n",
        "cell.value = \"VSL\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=5, start_column=3, end_row=5, end_column=5)\n",
        "cell = ws.cell(row=5, column=3)\n",
        "cell.value = \"TIER 1\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "# Helper function to detect if a string contains English letters\n",
        "def contains_english(value):\n",
        "    return any(char.isascii() and char.isalpha() for char in str(value))\n",
        "\n",
        "# Step 2: Add the 'name' column to the first column in the Excel file starting from row 4 of df_sorted\n",
        "for row_idx, value in enumerate(df_sorted[\"name\"][4:], start=6):\n",
        "    cell = ws.cell(row=row_idx, column=1)\n",
        "    cell.value = value\n",
        "    cell.font = Font(\n",
        "        name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "        size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "    )\n",
        "    cell.alignment = alignment_center_wrap\n",
        "    cell.border = thin_border\n",
        "\n",
        "# Merge cells in column 1 with the same value\n",
        "current_start = 6\n",
        "previous_value = None\n",
        "for row_idx, value in enumerate(df_sorted[\"name\"][4:], start=6):\n",
        "    if value == previous_value:\n",
        "        continue\n",
        "    if previous_value is not None:\n",
        "        ws.merge_cells(start_row=current_start, start_column=1, end_row=row_idx - 1, end_column=1)\n",
        "    current_start = row_idx\n",
        "    previous_value = value\n",
        "# Merge the last group\n",
        "if previous_value is not None:\n",
        "    ws.merge_cells(start_row=current_start, start_column=1, end_row=len(df_sorted[\"name\"][4:]) + 5, end_column=1)\n",
        "\n",
        "# Step 3: Add the 'depth' column to column 4 in the Excel file starting from row 6\n",
        "for row_idx, value in enumerate(df_sorted[\"depth\"][4:], start=6):\n",
        "    cell = ws.cell(row=row_idx, column=4)\n",
        "    cell.value = value\n",
        "    cell.font = Font(\n",
        "        name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "        size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "    )\n",
        "    cell.alignment = alignment_center_wrap\n",
        "    cell.border = thin_border\n",
        "\n",
        "# Step 4: Add data from column 6 onward starting from row 1 in Excel\n",
        "for col_idx, col_name in enumerate(df_sorted.columns[5:], start=6):  # Iterate through columns 6 onward\n",
        "    # Add column header\n",
        "    header_cell = ws.cell(row=1, column=col_idx)\n",
        "    header_cell.value = col_name\n",
        "    header_cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "    header_cell.alignment = alignment_center_wrap\n",
        "    header_cell.border = thin_border\n",
        "\n",
        "    # Add column data and round float values\n",
        "    for row_idx, value in enumerate(df_sorted[col_name], start=2):  # Start from row 2 in Excel\n",
        "        cell = ws.cell(row=row_idx, column=col_idx)\n",
        "        try:\n",
        "            value_numeric = round(float(value), 3)  # Round to 3 decimal places\n",
        "            cell.value = value_numeric\n",
        "        except (ValueError, TypeError):\n",
        "            cell.value = value  # Non-numeric values remain unchanged\n",
        "\n",
        "        cell.font = Font(\n",
        "            name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "            size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "        )\n",
        "        cell.alignment = alignment_center_wrap\n",
        "        cell.border = thin_border\n",
        "\n",
        "        # Conditional formatting for rows 6 and onward\n",
        "        if row_idx >= 6 and isinstance(cell.value, (float, int)):\n",
        "            try:\n",
        "                row_4_value = float(ws.cell(row=4, column=col_idx).value)\n",
        "                row_5_value = float(ws.cell(row=5, column=col_idx).value)\n",
        "\n",
        "                if cell.value > row_4_value:\n",
        "                    cell.fill = yellow_fill\n",
        "                    cell.font = Font(bold=True, name=cell.font.name, size=cell.font.size)\n",
        "                if cell.value > row_5_value:\n",
        "                    cell.fill = custom_fill\n",
        "                    cell.font = Font(bold=True, name=cell.font.name, size=cell.font.size)\n",
        "            except (ValueError, TypeError):\n",
        "                pass  # Skip cells that cannot be compared numerically\n",
        "\n",
        "# Merge columns 2 and 3 for each row from row 6 onward\n",
        "for row_idx in range(6, len(df_sorted[\"name\"]) + 6):\n",
        "    ws.merge_cells(start_row=row_idx, start_column=2, end_row=row_idx, end_column=3)\n",
        "\n",
        "# Define the medium border\n",
        "medium_side = Side(style=\"medium\")\n",
        "\n",
        "# Define the range for the table\n",
        "start_row = 1  # Start from the first row\n",
        "end_row = len(df_sorted)+1  # Dynamically determine the last row based on the data\n",
        "start_col = 1  # Start from the first column\n",
        "end_col = ws.max_column  # Dynamically determine the last column based on the data\n",
        "\n",
        "# Apply medium border to the outer edges of the table\n",
        "for row in range(start_row, end_row + 1):\n",
        "    for col in range(start_col, end_col + 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "\n",
        "        # Apply borders to the outer edges only\n",
        "        if row == start_row:  # Top edge\n",
        "            cell.border = Border(top=medium_side, left=cell.border.left, right=cell.border.right, bottom=cell.border.bottom)\n",
        "        if row == end_row:  # Bottom edge\n",
        "            cell.border = Border(bottom=medium_side, left=cell.border.left, right=cell.border.right, top=cell.border.top)\n",
        "        if col == start_col:  # Left edge\n",
        "            cell.border = Border(left=medium_side, top=cell.border.top, bottom=cell.border.bottom, right=cell.border.right)\n",
        "        if col == end_col:  # Right edge\n",
        "            cell.border = Border(right=medium_side, top=cell.border.top, bottom=cell.border.bottom, left=cell.border.left)\n",
        "\n",
        "# Define the range for the first 5 rows\n",
        "start_row = 1\n",
        "end_row = 5\n",
        "start_col = 1\n",
        "end_col = ws.max_column  # Dynamically determine the last column based on the data\n",
        "\n",
        "# Apply thick border to the outer edges\n",
        "for row in range(start_row, end_row + 1):\n",
        "    for col in range(start_col, end_col + 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "\n",
        "        # Apply borders to the outer edges only\n",
        "        if row == start_row:  # Top edge\n",
        "            cell.border = Border(top=medium_side, left=cell.border.left, right=cell.border.right, bottom=cell.border.bottom)\n",
        "        if row == end_row:  # Bottom edge\n",
        "            cell.border = Border(bottom=medium_side, left=cell.border.left, right=cell.border.right, top=cell.border.top)\n",
        "        if col == start_col:  # Left edge\n",
        "            cell.border = Border(left=medium_side, top=cell.border.top, bottom=cell.border.bottom, right=cell.border.right)\n",
        "        if col == end_col:  # Right edge\n",
        "            cell.border = Border(right=medium_side, top=cell.border.top, bottom=cell.border.bottom, left=cell.border.left)\n",
        "\n",
        "# Determine the last row and column of the table\n",
        "last_row = ws.max_row\n",
        "start_col = 1  # Assuming you want the legend to start from the first column\n",
        "\n",
        "# Add legend for \"חורג מערך VSL\"\n",
        "legend_vsl_cell = ws.cell(row=len(df_sorted)+2 , column=start_col)\n",
        "legend_vsl_cell.value = \"חורג מערך VSL\"\n",
        "legend_vsl_cell.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_vsl_cell.alignment = alignment_center_wrap\n",
        "legend_vsl_cell.fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
        "legend_vsl_cell.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for \"חורג מערך TIER 1\"\n",
        "legend_tier1_cell = ws.cell(row=len(df_sorted)+3 , column=start_col)\n",
        "legend_tier1_cell.value = \"חורג מערך TIER 1\"\n",
        "legend_tier1_cell.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_tier1_cell.alignment = alignment_center_wrap\n",
        "legend_tier1_cell.fill = PatternFill(start_color=\"F7C7AC\", end_color=\"F7C7AC\", fill_type=\"solid\")\n",
        "legend_tier1_cell.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Save the final Excel file with the provided name\n",
        "file_path = f'{file_name}.xlsx'\n",
        "wb.save(file_path)\n",
        "\n",
        "# Download the Excel file\n",
        "files.download(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "3eMEbiLSSU4L",
        "outputId": "0449b39e-2438-40eb-f0de-4dcec6e81d10",
        "cellView": "form"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-34d7492877de>:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  results_data = results_data.applymap(remove_parentheses)\n",
            "<ipython-input-53-34d7492877de>:121: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  rows_to_sort['name_letter'] = rows_to_sort['name'].str.extract(r'([A-Za-z]+)', expand=False)\n",
            "<ipython-input-53-34d7492877de>:122: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  rows_to_sort['name_number'] = rows_to_sort['name'].str.extract(r'(\\d+)', expand=False).astype(float)\n",
            "<ipython-input-53-34d7492877de>:263: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['TPH DRO'][1] = \"C10-C40\"\n",
            "<ipython-input-53-34d7492877de>:264: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['TPH ORO'][1] = \"C10-C40\"\n",
            "<ipython-input-53-34d7492877de>:265: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['Total TPH'][1] = \"C10-C40\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_959bc878-fbe2-4840-8854-26b2c70c5360\", \"\\u05e0\\u05d9\\u05e1\\u05d9\\u05d5\\u05df.xlsx\", 7209)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}