{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***מחברת קוד לעיבוד נתוני תוצאות מכון הנפט - סקר קרקע***\n",
        "\n",
        "<bdi>\n",
        "\n",
        "ברירת המחדל של הטבלה הינה הצגת חריגות מערך ה-VSL.\n",
        "\n",
        "ניתן להציג חריגות מערך סף נוסף בהתאם למאפייני האתר.\n",
        "\n",
        "בתא השני נבחר את התרכובות אותם נרצה להציג בטבלת התוצאות. לדוגמה: TPH, MTBE BTEX וכו'.\n",
        "\n",
        "סימון האופציה **Add_exceptations** יציג בטבלה את כל תרכובות ה-VOC בהם יש חריגות מערכי הסף הנבחרים, VSL ו-TIER 1.\n",
        "\n",
        "ב-**Add_specifics** נוסיף מספרי CAS של תרכובות נוספות אותן נרצה להציג על אף שלא אותרו בהן חריגות מערכי הסף.\n",
        "\n",
        "\n",
        "על מנת לעשות זאת, יש לרשום במקום המיועד לכך לפי הפורמט הבא:\n",
        "\n",
        "'cas number, cas number'\n",
        "\n",
        " לדוגמה: '106-93-4, 74-83-9'\n",
        "\n",
        " ניתן הוסיף כמה אנליזות שרוצים, 1,2,3 וכו'.\n",
        "\n",
        " אם תבוקש אנליזה שלא נמצאת בתוצאות מעבדה תקפוץ הודעה, יש ללחוץ אישור ולבדוק איזה אנליזה לא באמת קיימת בקובץ תוצאות שהעלתם\n",
        "\n",
        "</bdi>"
      ],
      "metadata": {
        "id": "gAKKipm41-F1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4dpFJP75WgVI",
        "outputId": "64081103-0ebb-4a63-b2ac-4128483e3d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "#@title נא לבחור פרמטרים מתאימים  { run: \"auto\" }\n",
        "\n",
        "Tier1_type = \"Tier 1 Residential A 0-6m\" # @param [\"Tier 1 Residential A 0-6m\",\"Tier 1 Residential A >6m\", \"Tier 1 Residential B\",\"Tier 1 industrial A 0-6m\",\"Tier 1 industrial A >6m\",\"Tier 1 industrial B\"]\n",
        "\n",
        "testsׁ_basic = \"TPHּּּ + MTBE BTEX\" # @param [\"TPH\",\"MTBE BTEX\",\"TPHּּּ + MTBE BTEX\"]\n",
        "\n",
        "Add_exceptations_VOCs = True # @param {type:\"boolean\"}\n",
        "\n",
        "Add_specifics_VOCs = None # @param {type:\"raw\"}\n",
        "\n",
        "file_name = 'גל נתניה' # @param {type:\"string\"}\n",
        "\n",
        "Font_size_hebrew = 9 # @param {type:\"slider\", min:6, max:24, step:1}\n",
        "\n",
        "Font_name_hebrew = 'David' # @param {type:\"string\"}\n",
        "\n",
        "Font_size_english = 8 # @param {type:\"slider\", min:6, max:24, step:1}\n",
        "\n",
        "Font_name_english = 'Times New Roman' # @param {type:\"string\"}\n",
        "\n",
        "!pip install openpyxl\n",
        "from openpyxl.styles import Font\n",
        "import pandas as pd\n",
        "import re\n",
        "from IPython.display import Javascript\n",
        "\n",
        "if Add_specifics_VOCs == None:\n",
        "  Add_specifics_VOCs = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title להעלות קובץ ערכי סף של קרקע בלחיצה על הכפתור למטה\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "filenames = list(uploaded.keys())\n",
        "\n",
        "# Initialize an empty list to store the dataframes\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file and read it based on its extension\n",
        "for filename in filenames:\n",
        "    if filename.endswith('.csv'):\n",
        "        df = pd.read_csv(filename)\n",
        "    elif filename.endswith('.xlsx'):\n",
        "        df = pd.read_excel(filename)\n",
        "    else:\n",
        "        print(f\"Unsupported file format: {filename}\")\n",
        "        continue\n",
        "        # Round all numeric cells to 2 decimal points\n",
        "    #df = df.applymap(lambda x: round(x, 2) if isinstance(x, (int, float)) else x)\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all dataframes into one\n",
        "if dataframes:\n",
        "    threshold_data = pd.concat(dataframes, ignore_index=True)\n",
        "else:\n",
        "    print(\"No valid files uploaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "cellView": "form",
        "id": "c3tohGf-rAhP",
        "outputId": "4f490bed-df3f-4517-cdd8-55bb8e8e9ef0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-88638dd2-7167-497d-a81f-445ff4052bcd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-88638dd2-7167-497d-a81f-445ff4052bcd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving גרסה 7 דצמבר 2024 - soil_threshold.csv to גרסה 7 דצמבר 2024 - soil_threshold (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title להעלות קובץ תוצאות סקר קרקע של מכון הנפט בלחיצה על הכפתור למטה\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "filenames = list(uploaded.keys())\n",
        "\n",
        "# Initialize an empty list to store the dataframes\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file and read it based on its extension\n",
        "for filename in filenames:\n",
        "    if filename.endswith('.csv'):\n",
        "        df = pd.read_csv(filename)\n",
        "    elif filename.endswith('.xlsx'):\n",
        "        df = pd.read_excel(filename)\n",
        "    else:\n",
        "        print(f\"Unsupported file format: {filename}\")\n",
        "        continue\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all dataframes into one\n",
        "if dataframes:\n",
        "    df_res_all = pd.concat(dataframes, ignore_index=True)\n",
        "else:\n",
        "    print(\"No valid files uploaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "UdxL50sk7Jcg",
        "outputId": "b6acc7b1-fc1b-4f61-93d9-216edabfae93",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6a6d55cb-95ed-4050-8ca6-8175c95d6801\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6a6d55cb-95ed-4050-8ca6-8175c95d6801\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 4949 VOC.xlsx to 4949 VOC.xlsx\n",
            "Saving 5062 סונול איילון TPH+D+O+MTBE+BTEX.xlsx to 5062 סונול איילון TPH+D+O+MTBE+BTEX (1).xlsx\n",
            "Saving 5102 סונול אור יהודה VOC.xlsx to 5102 סונול אור יהודה VOC (1).xlsx\n",
            "Saving 5103 סונול המלאכה TPH+D+O+MBTE+BTEX.xlsx to 5103 סונול המלאכה TPH+D+O+MBTE+BTEX.xlsx\n",
            "Saving 5207 סונול רמלה יינה TPH+D+O.xlsx to 5207 סונול רמלה יינה TPH+D+O (2).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title יש להריץ את התא ולהוריד את הטבלה המוכנה\n",
        "\n",
        "def normalize_drill_name(item):\n",
        "    if pd.isna(item):\n",
        "        return item\n",
        "\n",
        "    # Match patterns where the number in parentheses can be at the start or end\n",
        "    match = re.match(r'\\((\\d+(\\.\\d+)?)\\)\\s*(.+)', item)\n",
        "    if match:\n",
        "        # Format as 'name (number)'\n",
        "        return f\"{match.group(3).strip()} ({match.group(1)})\"\n",
        "    else:\n",
        "        # Check if it's already in correct format or in reverse\n",
        "        match = re.match(r'(.+?)\\s*\\((\\d+(\\.\\d+)?)\\)', item)\n",
        "        if match:\n",
        "            return f\"{match.group(1).strip()} ({match.group(2)})\"\n",
        "        else:\n",
        "            # If no match, keep the item as is\n",
        "            return item.strip()\n",
        "\n",
        "results_data_TPH= pd.DataFrame()\n",
        "results_data_MTBE= pd.DataFrame()\n",
        "results_data_VOC= pd.DataFrame()\n",
        "results_data_VOC_op = pd.DataFrame()\n",
        "\n",
        "# Define the structure of the DataFrame\n",
        "columns = ['name', 'date', 'Unnamed: 2', 'depth', ' PID','full name']\n",
        "data = [\n",
        "    ['units', None, None, 'm', 'ppm', None],\n",
        "    ['Cas', None, None, None, None, None],\n",
        "    ['VSL', None, None, None, None, None],\n",
        "    ['TIER 1', None, None, None, None, None]\n",
        "]\n",
        "\n",
        "# Create the DataFrame\n",
        "df_template = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "drills_list= []\n",
        "\n",
        "if 'TPH' in testsׁ_basic:\n",
        "    df_res_all_cleaned = df_res_all.reset_index(drop=True)\n",
        "\n",
        "    collected_blocks = []  # To collect all TPH data blocks\n",
        "    i = 0\n",
        "\n",
        "    while i < len(df_res_all_cleaned):\n",
        "        row = df_res_all_cleaned.iloc[i]\n",
        "\n",
        "        # Look for 'בדיקה' in the row\n",
        "        if 'בדיקה' in row.values:\n",
        "            col_name_בדיקה = row[row == 'בדיקה'].index[0]\n",
        "            col_index_בדיקה = df_res_all_cleaned.columns.get_loc(col_name_בדיקה)\n",
        "            col_index_tph = col_index_בדיקה + 2\n",
        "\n",
        "            # Confirm 'TPH' exists two columns to the right\n",
        "            if col_index_tph < len(df_res_all_cleaned.columns):\n",
        "                val = row.iloc[col_index_tph]\n",
        "                if isinstance(val, str) and val.strip() == 'TPH':\n",
        "                    selected_columns = [\n",
        "                        df_res_all_cleaned.columns[col_index_בדיקה],       # 'בדיקה'\n",
        "                        df_res_all_cleaned.columns[col_index_tph],         # 'TPH'\n",
        "                        df_res_all_cleaned.columns[col_index_tph + 1],     # Next column\n",
        "                        df_res_all_cleaned.columns[col_index_tph + 2],     # Next column\n",
        "                    ]\n",
        "\n",
        "                    start_row = i + 4\n",
        "                    end_row = start_row\n",
        "\n",
        "                    # Find the end of this block\n",
        "                    for j in range(start_row, len(df_res_all_cleaned)):\n",
        "                        if pd.isna(df_res_all_cleaned.iloc[j, col_index_בדיקה]):\n",
        "                            break\n",
        "                        end_row += 1\n",
        "\n",
        "                    block = df_res_all_cleaned.loc[start_row:end_row - 1, selected_columns].copy()\n",
        "\n",
        "                    # Rename and reorder columns\n",
        "                    block.columns = ['Analyte', 'Total TPH', 'DRO', 'ORO']\n",
        "                    block = block[['Analyte', 'DRO', 'ORO', 'Total TPH']]\n",
        "                    block['Analyte'] = block['Analyte'].apply(normalize_drill_name)\n",
        "\n",
        "                    collected_blocks.append(block)\n",
        "\n",
        "                    # Move to end of current block\n",
        "                    i = end_row\n",
        "                    continue\n",
        "        i += 1\n",
        "\n",
        "    if collected_blocks:\n",
        "        results_data_TPH = pd.concat(collected_blocks, ignore_index=True)\n",
        "        drills_list = results_data_TPH[\"Analyte\"].dropna().tolist()\n",
        "    else:\n",
        "        display(Javascript('alert(\"⚠️ TPH is not in the file results you uploaded ⚠️\")'))\n",
        "\n",
        "if 'MTBE' in testsׁ_basic:\n",
        "    df_res_all_cleaned = df_res_all.reset_index(drop=True)\n",
        "    collected_blocks = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(df_res_all_cleaned):\n",
        "        row = df_res_all_cleaned.iloc[i]\n",
        "\n",
        "        if 'בדיקה' in row.values:\n",
        "            col_name_בדיקה = row[row == 'בדיקה'].index[0]\n",
        "            col_index_בדיקה = df_res_all_cleaned.columns.get_loc(col_name_בדיקה)\n",
        "            col_index_mtbe = col_index_בדיקה + 2\n",
        "\n",
        "            if col_index_mtbe < len(df_res_all_cleaned.columns):\n",
        "                val = row.iloc[col_index_mtbe]\n",
        "                if isinstance(val, str) and val.strip() == 'MTBE':\n",
        "                    selected_columns = [\n",
        "                        df_res_all_cleaned.columns[col_index_בדיקה],       # 'בדיקה'\n",
        "                        df_res_all_cleaned.columns[col_index_mtbe],        # MTBE\n",
        "                        df_res_all_cleaned.columns[col_index_mtbe + 1],    # Benzene\n",
        "                        df_res_all_cleaned.columns[col_index_mtbe + 2],    # Toluene\n",
        "                        df_res_all_cleaned.columns[col_index_mtbe + 3],    # Ethylbenzene\n",
        "                        df_res_all_cleaned.columns[col_index_mtbe + 4],    # Xylene\n",
        "                    ]\n",
        "\n",
        "                    start_row = i + 6\n",
        "                    end_row = start_row\n",
        "\n",
        "                    for j in range(start_row, len(df_res_all_cleaned)):\n",
        "                        if pd.isna(df_res_all_cleaned.iloc[j, col_index_בדיקה]):\n",
        "                            break\n",
        "                        end_row += 1\n",
        "\n",
        "                    block = df_res_all_cleaned.loc[\n",
        "                        [start_row - 4] + list(range(start_row, end_row)),\n",
        "                        selected_columns\n",
        "                    ].copy()\n",
        "\n",
        "                    block.columns = ['Analyte', 'MTBE', 'Benzene', 'Toluene', 'Ethylbenzene', 'Xylene']\n",
        "                    collected_blocks.append(block)\n",
        "\n",
        "                    i = end_row\n",
        "                    continue\n",
        "        i += 1\n",
        "\n",
        "    if collected_blocks:\n",
        "        results_data_MTBE = pd.concat(collected_blocks, ignore_index=True)\n",
        "\n",
        "        if drills_list == []:\n",
        "            results_data_MTBE['Analyte'] = results_data_MTBE['Analyte'].apply(normalize_drill_name)\n",
        "            drills_list = results_data_MTBE[\"Analyte\"].dropna().tolist()[1:]\n",
        "        else:\n",
        "            results_data_MTBE['Analyte'] = results_data_MTBE['Analyte'].apply(normalize_drill_name)\n",
        "            mtbe_drills_list = results_data_MTBE[\"Analyte\"].dropna().tolist()[1:]\n",
        "\n",
        "            for mtbe_item in mtbe_drills_list:\n",
        "                if not any(mtbe_item in drill_item for drill_item in drills_list):\n",
        "                    drills_list.append(mtbe_item)\n",
        "\n",
        "        results_data_MTBE = results_data_MTBE.reset_index(drop=True)\n",
        "\n",
        "        def replace_ND(cell, column):\n",
        "            if isinstance(cell, str) and \"ND\" in cell:\n",
        "                return f\"<{results_data_MTBE.iloc[0, column]}\"\n",
        "            return cell\n",
        "\n",
        "        for col in results_data_MTBE.columns[1:]:\n",
        "            results_data_MTBE[col] = results_data_MTBE[col].apply(\n",
        "                lambda x: replace_ND(x, results_data_MTBE.columns.get_loc(col))\n",
        "            )\n",
        "\n",
        "        results_data_MTBE = results_data_MTBE.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        if (df_res_all == '1634-04-4').any().any():\n",
        "            df_res_all_cleaned = df_res_all.reset_index(drop=True)\n",
        "            results_data_VOCs_dict = {}\n",
        "            occurrence_count = 0\n",
        "\n",
        "            for row_index, row in df_res_all_cleaned.iterrows():\n",
        "                if 'ב ד י ק ה' in row.values:\n",
        "                    col_name_בדיקה = row[row == 'ב ד י ק ה'].index[0]\n",
        "                    col_index_בדיקה = df_res_all_cleaned.columns.get_loc(col_name_בדיקה)\n",
        "                    col_index_גילוי = col_index_בדיקה + 4\n",
        "\n",
        "                    if col_index_גילוי < len(df_res_all_cleaned.columns) and row.iloc[col_index_גילוי].strip() == 'גבול גילוי':\n",
        "                        בדיקה_row = row_index\n",
        "                        start_row = בדיקה_row + 3\n",
        "                        end_row = start_row\n",
        "\n",
        "                        for i in range(start_row, len(df_res_all_cleaned)):\n",
        "                            value = df_res_all_cleaned.iloc[i, col_index_בדיקה + 1]\n",
        "                            if pd.isna(value):\n",
        "                                break\n",
        "                            end_row += 1\n",
        "\n",
        "                        start_col = col_index_גילוי + 2\n",
        "                        end_col = start_col\n",
        "\n",
        "                        for i in range(start_col, len(df_res_all_cleaned.columns)):\n",
        "                            value = df_res_all_cleaned.iloc[בדיקה_row + 1, i]\n",
        "                            if pd.isna(value):\n",
        "                                break\n",
        "                            end_col += 1\n",
        "\n",
        "                        results_data_VOC = df_res_all_cleaned.iloc[\n",
        "                            list(range(בדיקה_row + 1, end_row)),\n",
        "                            list(range(col_index_בדיקה + 1, end_col))\n",
        "                        ].reset_index(drop=True)\n",
        "\n",
        "                        def replace_ND(cell, row_index):\n",
        "                            if isinstance(cell, str) and \"ND\" in cell:\n",
        "                                return f\"<{results_data_VOC.iloc[row_index, 3]}\"\n",
        "                            return cell\n",
        "\n",
        "                        for col in results_data_VOC.columns:\n",
        "                            if results_data_VOC.columns.get_loc(col) != 3:\n",
        "                                results_data_VOC[col] = results_data_VOC.apply(\n",
        "                                    lambda row: replace_ND(row[col], row.name), axis=1\n",
        "                                )\n",
        "\n",
        "                        results_data_VOC_op = results_data_VOC.T.reset_index(drop=True)\n",
        "                        results_data_VOC_op = results_data_VOC_op.drop(index=range(2, 5))\n",
        "                        results_data_VOC_op = results_data_VOC_op.drop(columns=results_data_VOC_op.columns[1])\n",
        "                        results_data_VOC_op = results_data_VOC_op.reset_index(drop=True)\n",
        "\n",
        "                        if occurrence_count > 0:\n",
        "                            results_data_VOC_op = results_data_VOC_op.drop(index=range(0, 2))\n",
        "\n",
        "                        results_data_VOCs_dict[f\"results_data_VOC_{occurrence_count}\"] = results_data_VOC_op\n",
        "                        occurrence_count += 1\n",
        "\n",
        "            if results_data_VOCs_dict:\n",
        "                results_data_VOC_op = pd.concat(results_data_VOCs_dict.values(), ignore_index=True)\n",
        "                results_data_VOC_op = results_data_VOC_op.drop(index=1)\n",
        "\n",
        "                cas_mapping = {\n",
        "                    'MTBE': '1634-04-4',\n",
        "                    'Benzene': '71-43-2',\n",
        "                    'Toluene': '108-88-3',\n",
        "                    'Ethylbenzene': '100-41-4',\n",
        "                    'Xylene': '95-47-6'\n",
        "                }\n",
        "\n",
        "                cas_numbers = list(cas_mapping.values())\n",
        "                columns_to_keep = [results_data_VOC_op.columns[0]]\n",
        "                columns_to_keep += [\n",
        "                    col for col in results_data_VOC_op.columns[1:]\n",
        "                    if results_data_VOC_op.at[0, col] in cas_numbers\n",
        "                ]\n",
        "                results_data_VOC_op = results_data_VOC_op[columns_to_keep]\n",
        "\n",
        "                for chemical, cas_number in cas_mapping.items():\n",
        "                    results_data_VOC_op.replace(cas_number, chemical, inplace=True)\n",
        "\n",
        "                results_data_VOC_op.columns = results_data_VOC_op.iloc[0]\n",
        "                results_data_VOC_op = results_data_VOC_op[1:]\n",
        "                results_data_VOC_op.rename(columns={results_data_VOC_op.columns[0]: 'Analyte'}, inplace=True)\n",
        "                desired_order = ['Analyte', 'MTBE', 'Benzene', 'Toluene', 'Ethylbenzene', 'Xylene']\n",
        "                results_data_VOC_op = results_data_VOC_op.reindex(columns=desired_order)\n",
        "\n",
        "                results_data_MTBE = results_data_VOC_op\n",
        "\n",
        "                if drills_list == []:\n",
        "                    results_data_MTBE['Analyte'] = results_data_MTBE['Analyte'].apply(normalize_drill_name)\n",
        "                    drills_list = results_data_MTBE[\"Analyte\"].dropna().tolist()[1:]\n",
        "                else:\n",
        "                    results_data_MTBE['Analyte'] = results_data_MTBE['Analyte'].apply(normalize_drill_name)\n",
        "                    mtbe_drills_list = results_data_MTBE[\"Analyte\"].dropna().tolist()[1:]\n",
        "                    for mtbe_item in mtbe_drills_list:\n",
        "                        if not any(mtbe_item in drill_item for drill_item in drills_list):\n",
        "                            drills_list.append(mtbe_item)\n",
        "\n",
        "                results_data_VOC = results_data_VOC_op.T\n",
        "                results_data_VOC.iloc[:, [0, 1]] = results_data_VOC.iloc[:, [1, 0]].values\n",
        "\n",
        "        else:\n",
        "            display(Javascript('alert(\"⚠️ MTBE BTEX is not in the file results you uploaded ⚠️\")'))\n",
        "\n",
        "if Add_exceptations_VOCs == True:\n",
        "    df_res_all_cleaned = df_res_all.reset_index(drop=True)\n",
        "\n",
        "    # Initialize a list to store DataFrames for each occurrence\n",
        "    results_data_VOCs_dict = {}\n",
        "    occurrence_count = 0\n",
        "\n",
        "    # Dynamically find the row containing 'בדיקה' and ensure 'גבול גילוי' exists four columns to the right\n",
        "    start_row = None\n",
        "    col_index_בדיקה = None\n",
        "\n",
        "    for row_index, row in df_res_all_cleaned.iterrows():\n",
        "        if 'ב ד י ק ה' in row.values:\n",
        "            col_name_בדיקה = row[row == 'ב ד י ק ה'].index[0]  # Get column name of 'בדיקה'\n",
        "            col_index_בדיקה = df_res_all_cleaned.columns.get_loc(col_name_בדיקה)  # Get column index of 'בדיקה'\n",
        "            col_index_גילוי = col_index_בדיקה + 4  # Four columns to the right\n",
        "\n",
        "            # Validate if 'גבול גילוי' exists at the correct position (strip whitespace)\n",
        "            if col_index_גילוי < len(df_res_all_cleaned.columns) and row.iloc[col_index_גילוי].strip() == 'גבול גילוי':\n",
        "                start_row = row_index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                בדיקה_row = start_row\n",
        "\n",
        "                # Find the end row by stopping at the first NaN in the 'בדיקה' column\n",
        "                start_row += 3\n",
        "                end_row = start_row\n",
        "\n",
        "                for i in range(start_row, len(df_res_all_cleaned)):\n",
        "                    value = df_res_all_cleaned.iloc[i, col_index_בדיקה + 1]\n",
        "                    if pd.isna(value):  # Stop at the first empty cell\n",
        "                        break\n",
        "                    end_row += 1\n",
        "\n",
        "                # Find the end column by stopping at the first NaN in the relevant row\n",
        "                start_col = col_index_גילוי + 2\n",
        "                end_col = start_col\n",
        "\n",
        "                for i in range(start_col, len(df_res_all_cleaned.columns)):\n",
        "                    value = df_res_all_cleaned.iloc[בדיקה_row + 1, i]\n",
        "                    if pd.isna(value):  # Stop at the first empty cell\n",
        "                        break\n",
        "                    end_col += 1\n",
        "\n",
        "                # Extract the relevant rows and columns dynamically using `.iloc`\n",
        "                results_data_VOC = df_res_all_cleaned.iloc[\n",
        "                    list(range(בדיקה_row + 1, end_row)),\n",
        "                    list(range(col_index_בדיקה +1, end_col))\n",
        "                ]\n",
        "\n",
        "                results_data_VOC = results_data_VOC.reset_index(drop=True)\n",
        "                results_data_VOC.iloc[0] = results_data_VOC.iloc[0].apply(normalize_drill_name)\n",
        "\n",
        "                # Define the function to replace \"ND\" based on the value in column 4\n",
        "                def replace_ND(cell, row_index):\n",
        "                    if isinstance(cell, str) and \"ND\" in cell:\n",
        "                        return f\"<{results_data_VOC.iloc[row_index, 3]}\"  # Replace with the value in column 3 of the same row\n",
        "                    return cell\n",
        "\n",
        "                # Iterate over each column (excluding column 3 itself) and apply the replacement\n",
        "                for col in results_data_VOC.columns:  # Iterate over all columns\n",
        "                    if results_data_VOC.columns.get_loc(col) != 3:  # Skip column 3\n",
        "                        results_data_VOC[col] = results_data_VOC.apply(\n",
        "                            lambda row: replace_ND(row[col], row.name), axis=1\n",
        "                        )\n",
        "\n",
        "                results_data_VOC_op = results_data_VOC.T\n",
        "                results_data_VOC_op = results_data_VOC_op.reset_index(drop=True)\n",
        "\n",
        "                # Drop the 1-4 rows\n",
        "                results_data_VOC_op = results_data_VOC_op.drop(index=range(2, 5))\n",
        "                results_data_VOC_op = results_data_VOC_op.drop(columns=results_data_VOC_op.columns[1])\n",
        "                results_data_VOC_op = results_data_VOC_op.reset_index(drop=True)\n",
        "                #results_data_VOC_op[0] = results_data_VOC_op[0].apply(normalize_drill_name)\n",
        "\n",
        "                if occurrence_count > 0:\n",
        "                  results_data_VOC_op = results_data_VOC_op.drop(index=range(0,2))\n",
        "\n",
        "                # Save the DataFrame with a unique name in the dictionary\n",
        "                results_data_VOCs_dict[f\"results_data_VOC_{occurrence_count}\"] = results_data_VOC_op\n",
        "\n",
        "                # Increment the counter for the next DataFrame\n",
        "                occurrence_count += 1\n",
        "    if results_data_VOCs_dict:  # Check if there are any DataFrames to concatenate\n",
        "    # Concatenate all DataFrames in the dictionary as new rows\n",
        "      results_data_VOC_op = pd.concat(results_data_VOCs_dict.values(), ignore_index=True)\n",
        "\n",
        "    if drills_list == []:\n",
        "      #results_data_VOC_op[0] = results_data_VOC_op[0].apply(normalize_drill_name)\n",
        "      drills_list = results_data_VOC_op[0].dropna().tolist()\n",
        "      #drills_list = normalize_drills_list(drills_list)\n",
        "\n",
        "      #drills_list = drills_list[1:]\n",
        "    if not results_data_VOC_op.empty:\n",
        "      #results_data_VOC_op[0] = results_data_VOC_op[0].apply(normalize_drill_name)\n",
        "      VOC_drills_list = results_data_VOC_op[0].dropna().tolist()\n",
        "      #VOC_drills_list = normalize_drills_list(VOC_drills_list)\n",
        "\n",
        "\n",
        "      for VOC_item in VOC_drills_list:\n",
        "          # Check if voc_item is in any item of drills_list\n",
        "          if not any(VOC_item in drill_item for drill_item in drills_list):\n",
        "              drills_list.append(VOC_item)  # Add to drills_list if no match is found\n",
        "\n",
        "      results_data_VOC = results_data_VOC_op.T\n",
        "      results_data_VOC.iloc[:, [0, 1]] = results_data_VOC.iloc[:, [1, 0]].values\n",
        "      results_data_VOC_op = results_data_VOC_op.drop(index=0)\n",
        "\n",
        "    # Raise an error if 'בדיקה' with 'גבול גילוי' was not found\n",
        "    if start_row is None:\n",
        "        display(Javascript('alert(\"⚠️ VOC is not in the file results you uploaded ⚠️\")'))\n",
        "\n",
        "# Process each item in drills_list\n",
        "df_template['full name'] = None\n",
        "\n",
        "# Updated extraction logic for name, depth, and type\n",
        "def extract_name_and_depth(item):\n",
        "    if not isinstance(item, str):\n",
        "        return item, None, None\n",
        "\n",
        "    # Detect flags\n",
        "    is_dup = bool(re.search(r'\\b(?:dup|duplicate|DUP|D)\\b', item, re.IGNORECASE))\n",
        "    is_split = bool(re.search(r'\\b(?:split|SP)\\b', item, re.IGNORECASE))\n",
        "\n",
        "    # Clean flags from item for name extraction\n",
        "    clean_item = re.sub(r'\\b(?:dup|duplicate|DUP|D|split|SP)\\b', '', item, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Match name and depth\n",
        "    match_paren = re.search(r'(.+?)\\s*\\((\\d+\\.\\d+)\\)', clean_item)\n",
        "    if match_paren:\n",
        "        name = match_paren.group(1).strip()\n",
        "        depth = match_paren.group(2)\n",
        "    else:\n",
        "        match_dash = re.search(r'^(.+?-\\d+\\.\\d+)\\s*-\\s*(\\d+\\.\\d+)', clean_item)\n",
        "        if match_dash:\n",
        "            name = match_dash.group(1).strip()\n",
        "            depth = match_dash.group(2)\n",
        "        else:\n",
        "            match = re.findall(r'(\\d+\\.\\d+)', clean_item)\n",
        "            if match:\n",
        "                depth = match[-1]\n",
        "                name = clean_item[:clean_item.rfind(depth)].strip(\" -\")\n",
        "            else:\n",
        "                name, depth = clean_item.strip(), None\n",
        "\n",
        "    return name, depth, ('**' if is_split else '*') if is_dup or is_split else ''\n",
        "\n",
        "# Build a temp list to group regular, DUP, and split rows\n",
        "temp_rows = {}\n",
        "\n",
        "for item in drills_list:\n",
        "    name, depth, suffix = extract_name_and_depth(item)\n",
        "    if depth is None:\n",
        "        continue\n",
        "    key = (name, depth)\n",
        "\n",
        "    # Initialize group\n",
        "    if key not in temp_rows:\n",
        "        temp_rows[key] = []\n",
        "\n",
        "    # Compose adjusted depth\n",
        "    depth_display = f\"{depth}{suffix}\"\n",
        "    temp_rows[key].append({\n",
        "        'name': name,\n",
        "        'depth': depth_display,\n",
        "        'full name': item\n",
        "    })\n",
        "\n",
        "# Flatten into df_template in the order: original, DUP, split\n",
        "for key in sorted(temp_rows.keys()):\n",
        "    # Ensure order: normal -> * -> **\n",
        "    ordered = sorted(temp_rows[key], key=lambda x: len(x['depth']))\n",
        "    for row_data in ordered:\n",
        "        new_row = {**row_data}\n",
        "        df_template = pd.concat([df_template, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "# Ensure columns match\n",
        "df_template = df_template.reindex(columns=columns, fill_value=None)\n",
        "\n",
        "\n",
        "# Define the starting row for sorting\n",
        "start_row = 4\n",
        "\n",
        "# Extract the header rows (rows before start_row) and the rows to be sorted\n",
        "header_rows = df_template.iloc[:start_row]\n",
        "rows_to_sort = df_template.iloc[start_row:]\n",
        "\n",
        "# Extract base name (e.g., ק) and numeric part as float (e.g., 1.1 from ק-1.1)\n",
        "rows_to_sort['name_prefix'] = rows_to_sort['name'].str.extract(r'([^-\\d]+)', expand=False)\n",
        "rows_to_sort['name_number'] = rows_to_sort['name'].str.extract(r'(\\d+\\.\\d+|\\d+)', expand=False).astype(float)\n",
        "\n",
        "# Sort by prefix, then by numeric name, then by depth\n",
        "sorted_rows = rows_to_sort.sort_values(\n",
        "    by=['name_prefix', 'name_number', 'depth'],\n",
        "    key=lambda col: pd.to_numeric(col, errors='coerce') if col.name in ['name_number', 'depth'] else col\n",
        ")\n",
        "\n",
        "# Drop the temporary columns used for sorting\n",
        "sorted_rows = sorted_rows.drop(columns=['name_prefix', 'name_number'])\n",
        "\n",
        "# Concatenate the header rows with the sorted rows\n",
        "df_sorted = pd.concat([header_rows, sorted_rows], ignore_index=True)\n",
        "\n",
        "if 'TPH' in testsׁ_basic and not results_data_TPH.empty:\n",
        "  df_sorted['TPH DRO'] = None\n",
        "  df_sorted['TPH ORO'] = None\n",
        "  df_sorted['Total TP'] = None\n",
        "\n",
        "  # Ensure column names are consistent and merge based on \"full name\" and \"Analyte\"\n",
        "  merged_df = df_sorted.merge(\n",
        "      results_data_TPH.rename(columns={\"Analyte\": \"full name\"}),\n",
        "      on=\"full name\",\n",
        "      how=\"left\"\n",
        "  )\n",
        "\n",
        "  # Update the relevant columns in df_sorted with data from results_data_TPH\n",
        "  df_sorted['TPH DRO'] = merged_df['DRO']\n",
        "  df_sorted['TPH ORO'] = merged_df['ORO']\n",
        "  df_sorted['Total TP'] = merged_df['Total TPH']\n",
        "\n",
        "  # Renaming the column 'Total TP' to 'Total TPH'\n",
        "  df_sorted.rename(columns={'Total TP': 'Total TPH'}, inplace=True)\n",
        "\n",
        "  #Adding CAS number\n",
        "  df_sorted['TPH DRO'][1] = \"C10-C40\"\n",
        "  df_sorted['TPH ORO'][1] = \"C10-C40\"\n",
        "  df_sorted['Total TPH'][1] = \"C10-C40\"\n",
        "\n",
        "if 'MTBE' in testsׁ_basic and not results_data_MTBE.empty:\n",
        "  df_sorted['MTBE_1'] = None\n",
        "  df_sorted['Benzene_1'] = None\n",
        "  df_sorted['Toluene_1'] = None\n",
        "  df_sorted['Ethylbenzene_1'] = None\n",
        "  df_sorted['Xylene_1'] = None\n",
        "\n",
        "  # Ensure column names are consistent and merge based on \"full name\" and \"Analyte\"\n",
        "  merged_df = df_sorted.merge(\n",
        "      results_data_MTBE.rename(columns={\"Analyte\": \"full name\"}),\n",
        "      on=\"full name\",\n",
        "      how=\"left\"\n",
        "  )\n",
        "\n",
        "  # Update the relevant columns in df_sorted with data from results_data_TPH\n",
        "  df_sorted['MTBE_1'] = merged_df['MTBE']\n",
        "  df_sorted['Benzene_1'] = merged_df['Benzene']\n",
        "  df_sorted['Toluene_1'] = merged_df['Toluene']\n",
        "  df_sorted['Ethylbenzene_1'] = merged_df['Ethylbenzene']\n",
        "  df_sorted['Xylene_1'] = merged_df['Xylene']\n",
        "\n",
        "  # Renaming the columns\n",
        "  df_sorted.rename(columns={'MTBE_1': 'MTBE'}, inplace=True)\n",
        "  df_sorted.rename(columns={'Benzene_1': 'Benzene'}, inplace=True)\n",
        "  df_sorted.rename(columns={'Toluene_1': 'Toluene'}, inplace=True)\n",
        "  df_sorted.rename(columns={'Ethylbenzene_1': 'Ethylbenzene'}, inplace=True)\n",
        "  df_sorted.rename(columns={'Xylene_1': 'Xylene'}, inplace=True)\n",
        "\n",
        "  #Adding CAS number\n",
        "  df_sorted['MTBE'][1] = '1634-04-4'\n",
        "  df_sorted['Benzene'][1] = '71-43-2'\n",
        "  df_sorted['Toluene'][1] = '108-88-3'\n",
        "  df_sorted['Ethylbenzene'][1] = '100-41-4'\n",
        "  df_sorted['Xylene'][1] = '95-47-6'\n",
        "\n",
        "\n",
        "if Add_exceptations_VOCs == True and not results_data_VOC_op.empty:\n",
        "    # Ensure CAS numbers in both dataframes are properly formatted\n",
        "    results_data_VOC_op.iloc[0, :] = results_data_VOC_op.iloc[0, :].astype(str).str.strip()\n",
        "    threshold_data['CAS No.'] = threshold_data['CAS No.'].astype(str).str.strip()\n",
        "\n",
        "    # Initialize a new row with NaN values\n",
        "    new_row = [None] * results_data_VOC_op.shape[1]\n",
        "\n",
        "    # Iterate through each CAS number in the first row of results_data_VOC_op\n",
        "    for col_index, cas_number in enumerate(results_data_VOC_op.iloc[0, :]):\n",
        "        if pd.isna(cas_number) or cas_number == 'nan':\n",
        "            continue\n",
        "\n",
        "        # Find the matching row in threshold_data\n",
        "        matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "        if not matching_row.empty:\n",
        "            # Get the value from the \"VSL\" column\n",
        "            vsl_value = matching_row.iloc[0]['VSL']\n",
        "            new_row[col_index] = vsl_value\n",
        "\n",
        "    # Append the new row to results_data_VOC_op\n",
        "    results_data_VOC_op.loc[len(results_data_VOC_op) + 1] = new_row\n",
        "\n",
        "    ### Adding the values\n",
        "    # Iterate through each column in results_data_VOC_op\n",
        "    for col_index in range(results_data_VOC_op.shape[1]):\n",
        "        cas_number = results_data_VOC_op.iloc[0, col_index]  # Get the CAS number from row 0\n",
        "        if pd.isna(cas_number) or cas_number == 'nan':\n",
        "            continue\n",
        "\n",
        "        # Check if the CAS number is already in df_sorted\n",
        "        if cas_number in df_sorted.iloc[1, :].values:\n",
        "            continue  # Skip if the CAS number is already present\n",
        "\n",
        "        # Get the column values excluding the first row (CAS number)\n",
        "        column_values = results_data_VOC_op.iloc[1:, col_index]\n",
        "\n",
        "        # Skip columns where all values are NaN\n",
        "        if column_values.isna().all():\n",
        "            continue\n",
        "\n",
        "        # Filter column values to keep only numeric ones\n",
        "        numeric_values = pd.to_numeric(column_values, errors='coerce')\n",
        "\n",
        "        # Get the VSL value (assuming it's in the last row of the column)\n",
        "        vsl_value = results_data_VOC_op.iloc[-1, col_index]\n",
        "        if vsl_value is not None and not pd.isna(vsl_value):  # Check for None or NaN\n",
        "            try:\n",
        "                vsl_value = float(vsl_value)  # Ensure VSL value is a float\n",
        "            except ValueError:\n",
        "                continue  # Skip columns where VSL is not a valid number\n",
        "        else:\n",
        "            continue  # Skip if vsl_value is None or NaN\n",
        "\n",
        "        # **Check for numeric values exceeding VSL**\n",
        "        if (numeric_values > vsl_value).any():\n",
        "            matching_row = results_data_VOC[results_data_VOC.iloc[:, 1] == cas_number]\n",
        "            header_name = matching_row.iloc[0, 0] if not matching_row.empty else f\"Column_{cas_number}\"\n",
        "\n",
        "            # Add new column to df_sorted\n",
        "            df_sorted[header_name] = None\n",
        "            df_sorted.at[1, header_name] = cas_number\n",
        "\n",
        "            # Fill values where 'full name' matches\n",
        "            for index, row in df_sorted.iterrows():\n",
        "                if index <= 1:\n",
        "                    continue\n",
        "                name = row['full name']\n",
        "                if pd.isna(name):\n",
        "                    continue\n",
        "\n",
        "                matching_row = results_data_VOC_op[\n",
        "                    results_data_VOC_op[0].apply(lambda x: x in name if pd.notna(x) else False)\n",
        "                ]\n",
        "                if matching_row.empty:\n",
        "                    continue\n",
        "\n",
        "                value_to_fill = matching_row.iloc[0, col_index]\n",
        "                df_sorted.at[index, header_name] = value_to_fill\n",
        "\n",
        "        # **Check for \"<...\" values where the extracted number is greater than VSL**\n",
        "        for cell_value in column_values:\n",
        "            if isinstance(cell_value, str) and cell_value.startswith(\"<\"):\n",
        "                match = re.search(r'<\\s*(\\d+(\\.\\d+)?)', cell_value)\n",
        "                if match:\n",
        "                    try:\n",
        "                        extracted_value = float(match.group(1))  # Extracted number from \"<...\"\n",
        "\n",
        "                        if extracted_value > vsl_value:  # Compare with VSL\n",
        "                            # Add column if condition is met\n",
        "                            matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "                            header_name = (\n",
        "                                matching_row.iloc[0, -1]\n",
        "                                if not matching_row.empty\n",
        "                                else f\"Column_{cas_number}_BDL\"\n",
        "                            )\n",
        "\n",
        "                            # Ensure column exists in df_sorted\n",
        "                            if header_name not in df_sorted.columns:\n",
        "                                df_sorted[header_name] = None\n",
        "                                df_sorted.at[1, header_name] = cas_number\n",
        "\n",
        "                            # Fill values where 'full name' matches\n",
        "                            for index, row in df_sorted.iterrows():\n",
        "                                if index <= 1:\n",
        "                                    continue\n",
        "                                name = row['full name']\n",
        "                                if pd.isna(name):\n",
        "                                    continue\n",
        "\n",
        "                                matching_row = results_data_VOC_op[\n",
        "                                    results_data_VOC_op[0].apply(lambda x: x in name if pd.notna(x) else False)\n",
        "                                ]\n",
        "                                if matching_row.empty:\n",
        "                                    continue\n",
        "\n",
        "                                value_to_fill = matching_row.iloc[0, col_index]\n",
        "                                df_sorted.at[index, header_name] = value_to_fill\n",
        "\n",
        "                    except ValueError:\n",
        "                        pass  # Skip invalid values\n",
        "\n",
        "if Add_specifics_VOCs and not results_data_VOC_op.empty :\n",
        "    Add_specifics_VOCs_list = [item.strip() for item in Add_specifics_VOCs.split(',')]\n",
        "    for item in Add_specifics_VOCs_list:\n",
        "        if item not in results_data_VOC[1].values:\n",
        "            display(Javascript('alert(\"⚠️ one or more of the specific CAS numbers you insert is not in lab results ⚠️\")'))\n",
        "            print(f'{item} is not in lab results')\n",
        "        else:\n",
        "            # Find the column index of the item in results_data_VOC_op\n",
        "            try:\n",
        "                col_index = results_data_VOC_op.iloc[0, :].tolist().index(item)\n",
        "            except ValueError:\n",
        "                print(f'{item} not found in results_data_VOC_op header.')\n",
        "                continue\n",
        "\n",
        "            # Check if the CAS number (item) is already in df_sorted\n",
        "            if item in df_sorted.iloc[1, :].values:\n",
        "                continue  # Skip if the CAS number is already present\n",
        "\n",
        "            # Find the row in results_data_VOC where the CAS number matches\n",
        "            matching_row = results_data_VOC[\n",
        "                results_data_VOC.iloc[:, 1] == item  # Match CAS number in column 1\n",
        "            ]\n",
        "\n",
        "            # If a match is found, get the header from column 0 in the matching row\n",
        "            if not matching_row.empty:\n",
        "                header_name = matching_row.iloc[0, 0]\n",
        "            else:\n",
        "                header_name = f\"Column_{item}\"  # Fallback to a generic name\n",
        "\n",
        "            # Add a new column to df_sorted with the header_name\n",
        "            df_sorted[header_name] = None\n",
        "            df_sorted.at[1, header_name] = item\n",
        "\n",
        "            # Add matching values to df_sorted for the rows where 'full name' matches\n",
        "            for index, row in df_sorted.iterrows():\n",
        "                if index <= 1:  # Skip the header and CAS rows\n",
        "                    continue\n",
        "                name = row['full name']\n",
        "                if pd.isna(name):\n",
        "                    continue\n",
        "\n",
        "                # Find the matching row in results_data_VOC_op\n",
        "                matching_row = results_data_VOC_op[\n",
        "                    results_data_VOC_op[0].apply(lambda x: x in name if pd.notna(x) else False)\n",
        "                ]\n",
        "                if matching_row.empty:\n",
        "                    continue\n",
        "\n",
        "                # Extract the value from results_data_VOC_op for the current column\n",
        "                value_to_fill = matching_row.iloc[0, col_index]\n",
        "                df_sorted.at[index, header_name] = value_to_fill\n",
        "\n",
        "df_sorted.iloc[0, 6:] = 'mg/kg'\n",
        "\n",
        "# Iterate through each column in df_sorted starting from column 6 onward\n",
        "for col_index in range(6, len(df_sorted.columns)):\n",
        "    cas_number = df_sorted.iat[1, col_index]  # Get the CAS number from row 1\n",
        "\n",
        "    if pd.isna(cas_number) or cas_number == '':  # Skip if CAS number is empty or NaN\n",
        "        continue\n",
        "\n",
        "    # Find the matching row in threshold_data based on CAS No.\n",
        "    matching_row = threshold_data[threshold_data['CAS No.'] == cas_number]\n",
        "\n",
        "    if not matching_row.empty:\n",
        "        # Extract the VSL value from the matching row\n",
        "        vsl_value = matching_row.iloc[0]['VSL']\n",
        "        # Populate the cell in row 2 of df_sorted for the current column\n",
        "        df_sorted.iat[2, col_index] = vsl_value\n",
        "\n",
        "        # Extract the value from the column with the name Tier1_type\n",
        "        if Tier1_type in matching_row.columns:\n",
        "            tier1_value = matching_row.iloc[0][Tier1_type]\n",
        "            # Populate the cell in row 3 of df_sorted for the current column\n",
        "            df_sorted.iat[3, col_index] = tier1_value\n",
        "\n",
        "\n",
        "df_sorted = df_sorted.drop(columns='full name')\n",
        "\n",
        "# Add '*' to duplicate (name, depth) rows in df_sorted from row 5 onward\n",
        "seen_pairs = {}\n",
        "for idx in range(4, len(df_sorted)):\n",
        "    name = df_sorted.at[idx, 'name']\n",
        "    depth = str(df_sorted.at[idx, 'depth']).replace('*', '')  # strip existing stars\n",
        "    key = (name, depth)\n",
        "\n",
        "    if key in seen_pairs:\n",
        "        # If already seen, mark this one with *\n",
        "        df_sorted.at[idx, 'depth'] = f\"{depth}*\"\n",
        "    else:\n",
        "        seen_pairs[key] = idx  # store first occurrence\n",
        "\n",
        "########סידור לטבלת אקסל להורדה########\n",
        "\n",
        "# Fill empty values from row 5 (index 4) and column 6 onward with \"-\"\n",
        "for row_idx in range(4, len(df_sorted)):\n",
        "    for col_idx in range(5, len(df_sorted.columns)):\n",
        "        val = df_sorted.iat[row_idx, col_idx]\n",
        "        if pd.isna(val) or val == '':\n",
        "            df_sorted.iat[row_idx, col_idx] = \"-\"\n",
        "\n",
        "# Fill empty values in rows 3 and 4 (index 2 and 3) and column 6 onward with \"--\"\n",
        "for row_idx in [2, 3]:\n",
        "    for col_idx in range(5, len(df_sorted.columns)):\n",
        "        val = df_sorted.iat[row_idx, col_idx]\n",
        "        if pd.isna(val) or val == '':\n",
        "            df_sorted.iat[row_idx, col_idx] = \"--\"\n",
        "\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Border, Side, Alignment, PatternFill\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import pandas as pd\n",
        "\n",
        "# Create an Excel workbook and add a worksheet\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "\n",
        "# Set the sheet to right-to-left\n",
        "ws.sheet_view.rightToLeft = True\n",
        "\n",
        "thin_border = Border(\n",
        "    left=Side(style='thin'),\n",
        "    right=Side(style='thin'),\n",
        "    top=Side(style='thin'),\n",
        "    bottom=Side(style='thin')\n",
        ")\n",
        "\n",
        "alignment_center_wrap = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
        "\n",
        "# Define colors for conditional formatting\n",
        "yellow_fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
        "custom_fill = PatternFill(start_color=\"F7C7AC\", end_color=\"F7C7AC\", fill_type=\"solid\")\n",
        "gray_fill = PatternFill(start_color=\"D3D3D3\", end_color=\"D3D3D3\", fill_type=\"solid\")\n",
        "\n",
        "# Step 1: Add the merged cells for the headers\n",
        "ws.merge_cells(start_row=1, start_column=1, end_row=5, end_column=1)\n",
        "cell = ws.cell(row=1, column=1)\n",
        "cell.value = \"שם קידוח\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "for i in range(2, 6):\n",
        "  cell = ws.cell(row=i, column=1)\n",
        "  cell.border = Border(\n",
        "      left=Side(style='thin'),\n",
        "      right=Side(style='thin'))\n",
        "\n",
        "for i in range(2, 6):  # Iterate over columns (2 to 5 inclusive)\n",
        "    for j in range(1, len(df_sorted)+2):  # Iterate over rows (1 to len(df_sorted) - 1)\n",
        "        cell = ws.cell(row=j, column=i)\n",
        "        cell.border = Border(bottom=Side(style='thin'))\n",
        "\n",
        "ws.merge_cells(start_row=1, start_column=2, end_row=5, end_column=2)\n",
        "cell = ws.cell(row=1, column=2)\n",
        "cell.value = \"תאריך\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=3)\n",
        "cell.value = \"\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=3)\n",
        "cell.value = \"יחידות\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=4)\n",
        "cell.value = \"עומק\"\n",
        "cell.font = Font(name=Font_name_hebrew, size=Font_size_hebrew, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=4)\n",
        "cell.value = \"m\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=1, column=5)\n",
        "cell.value = \"PID\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "cell = ws.cell(row=2, column=5)\n",
        "cell.value = \"ppm\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=3, start_column=3, end_row=3, end_column=5)\n",
        "cell = ws.cell(row=3, column=3)\n",
        "cell.value = \"CAS\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=4, start_column=3, end_row=4, end_column=5)\n",
        "cell = ws.cell(row=4, column=3)\n",
        "cell.value = \"VSL\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "ws.merge_cells(start_row=5, start_column=3, end_row=5, end_column=5)\n",
        "cell = ws.cell(row=5, column=3)\n",
        "cell.value = \"TIER 1\"\n",
        "cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "cell.alignment = alignment_center_wrap\n",
        "cell.border = thin_border\n",
        "\n",
        "# Helper function to detect if a string contains English letters\n",
        "def contains_english(value):\n",
        "    return any(char.isascii() and char.isalpha() for char in str(value))\n",
        "\n",
        "# Step 2: Add the 'name' column to the first column in the Excel file starting from row 4 of df_sorted\n",
        "for row_idx, value in enumerate(df_sorted[\"name\"][4:], start=6):\n",
        "    cell = ws.cell(row=row_idx, column=1)\n",
        "    cell.value = value\n",
        "    cell.font = Font(\n",
        "        name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "        size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "    )\n",
        "    cell.alignment = alignment_center_wrap\n",
        "    cell.border = thin_border\n",
        "\n",
        "# Merge cells in column 1 with the same value\n",
        "current_start = 6\n",
        "previous_value = None\n",
        "for row_idx, value in enumerate(df_sorted[\"name\"][4:], start=6):\n",
        "    if value == previous_value:\n",
        "        continue\n",
        "    if previous_value is not None:\n",
        "        ws.merge_cells(start_row=current_start, start_column=1, end_row=row_idx - 1, end_column=1)\n",
        "    current_start = row_idx\n",
        "    previous_value = value\n",
        "# Merge the last group\n",
        "if previous_value is not None:\n",
        "    ws.merge_cells(start_row=current_start, start_column=1, end_row=len(df_sorted[\"name\"][4:]) + 5, end_column=1)\n",
        "\n",
        "# Step 3: Add the 'depth' column to column 4 in the Excel file starting from row 6\n",
        "for row_idx, value in enumerate(df_sorted[\"depth\"][4:], start=6):\n",
        "    cell = ws.cell(row=row_idx, column=4)\n",
        "    cell.value = value\n",
        "    cell.font = Font(\n",
        "        name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "        size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "    )\n",
        "    cell.alignment = alignment_center_wrap\n",
        "    cell.border = thin_border\n",
        "\n",
        "# Step 4: Add data from column 6 onward starting from row 1 in Excel\n",
        "for col_idx, col_name in enumerate(df_sorted.columns[5:], start=6):  # Iterate through columns 6 onward\n",
        "    # Add column header\n",
        "    header_cell = ws.cell(row=1, column=col_idx)\n",
        "    header_cell.value = col_name\n",
        "    header_cell.font = Font(name=Font_name_english, size=Font_size_english, bold=True)\n",
        "    header_cell.alignment = alignment_center_wrap\n",
        "    header_cell.border = thin_border\n",
        "\n",
        "    # Add column data and round float values\n",
        "    for row_idx, value in enumerate(df_sorted[col_name], start=2):  # Start from row 2 in Excel\n",
        "        cell = ws.cell(row=row_idx, column=col_idx)\n",
        "        try:\n",
        "            value_numeric = round(float(value), 4)  # Round to 4 decimal places\n",
        "            cell.value = value_numeric\n",
        "        except (ValueError, TypeError):\n",
        "            cell.value = value  # Non-numeric values remain unchanged\n",
        "\n",
        "        cell.font = Font(\n",
        "            name=Font_name_english if contains_english(value) else Font_name_hebrew,\n",
        "            size=Font_size_english if contains_english(value) else Font_size_hebrew,\n",
        "        )\n",
        "        cell.alignment = alignment_center_wrap\n",
        "        cell.border = thin_border\n",
        "\n",
        "        # Conditional formatting for rows 6 and onward\n",
        "        if row_idx >= 6:\n",
        "            try:\n",
        "                row_4_value = float(ws.cell(row=4, column=col_idx).value)\n",
        "                row_5_value = float(ws.cell(row=5, column=col_idx).value)\n",
        "\n",
        "                if isinstance(cell.value, (float, int)):\n",
        "                    if cell.value > row_4_value:\n",
        "                        cell.fill = yellow_fill\n",
        "                        cell.font = Font(bold=True, name=cell.font.name, size=cell.font.size)\n",
        "                    if cell.value > row_5_value:\n",
        "                        cell.fill = custom_fill\n",
        "                        cell.font = Font(bold=True, name=cell.font.name, size=cell.font.size)\n",
        "\n",
        "                # **Gray Fill Logic for \"<...\" values (Handles both Integers & Floats)**\n",
        "                elif isinstance(cell.value, str) and cell.value.startswith(\"<\"):\n",
        "                    match = re.search(r'<\\s*(\\d+(\\.\\d+)?)', cell.value)  # Extract numeric part\n",
        "\n",
        "                    if match:\n",
        "                        try:\n",
        "                            numeric_value = float(match.group(1))  # Convert extracted value to float\n",
        "                            threshold_value = ws.cell(row=4, column=col_idx).value  # Get row 4 value\n",
        "\n",
        "                            # Ensure threshold value is numeric before comparison\n",
        "                            if isinstance(threshold_value, (int, float)) and numeric_value > threshold_value:\n",
        "                                cell.fill = gray_fill\n",
        "                                cell.font = Font(bold=True, name=cell.font.name, size=cell.font.size)\n",
        "                        except ValueError:\n",
        "                            pass  # Skip if conversion fails\n",
        "\n",
        "            except (ValueError, TypeError):\n",
        "                pass  # Skip cells that cannot be compared numerically\n",
        "\n",
        "# Merge columns 2 and 3 for each row from row 6 onward\n",
        "for row_idx in range(6, len(df_sorted[\"name\"]) + 6):\n",
        "    ws.merge_cells(start_row=row_idx, start_column=2, end_row=row_idx, end_column=3)\n",
        "\n",
        "# Define the medium border\n",
        "medium_side = Side(style=\"medium\")\n",
        "\n",
        "# Define the range for the table\n",
        "start_row = 1  # Start from the first row\n",
        "end_row = len(df_sorted)+1  # Dynamically determine the last row based on the data\n",
        "start_col = 1  # Start from the first column\n",
        "end_col = ws.max_column  # Dynamically determine the last column based on the data\n",
        "\n",
        "# Apply medium border to the outer edges of the table\n",
        "for row in range(start_row, end_row + 1):\n",
        "    for col in range(start_col, end_col + 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "\n",
        "        # Apply borders to the outer edges only\n",
        "        if row == start_row:  # Top edge\n",
        "            cell.border = Border(top=medium_side, left=cell.border.left, right=cell.border.right, bottom=cell.border.bottom)\n",
        "        if row == end_row:  # Bottom edge\n",
        "            cell.border = Border(bottom=medium_side, left=cell.border.left, right=cell.border.right, top=cell.border.top)\n",
        "        if col == start_col:  # Left edge\n",
        "            cell.border = Border(left=medium_side, top=cell.border.top, bottom=cell.border.bottom, right=cell.border.right)\n",
        "        if col == end_col:  # Right edge\n",
        "            cell.border = Border(right=medium_side, top=cell.border.top, bottom=cell.border.bottom, left=cell.border.left)\n",
        "\n",
        "# Define the range for the first 5 rows\n",
        "start_row = 1\n",
        "end_row = 5\n",
        "start_col = 1\n",
        "end_col = ws.max_column  # Dynamically determine the last column based on the data\n",
        "\n",
        "# Apply thick border to the outer edges\n",
        "for row in range(start_row, end_row + 1):\n",
        "    for col in range(start_col, end_col + 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "\n",
        "        # Apply borders to the outer edges only\n",
        "        if row == start_row:  # Top edge\n",
        "            cell.border = Border(top=medium_side, left=cell.border.left, right=cell.border.right, bottom=cell.border.bottom)\n",
        "        if row == end_row:  # Bottom edge\n",
        "            cell.border = Border(bottom=medium_side, left=cell.border.left, right=cell.border.right, top=cell.border.top)\n",
        "        if col == start_col:  # Left edge\n",
        "            cell.border = Border(left=medium_side, top=cell.border.top, bottom=cell.border.bottom, right=cell.border.right)\n",
        "        if col == end_col:  # Right edge\n",
        "            cell.border = Border(right=medium_side, top=cell.border.top, bottom=cell.border.bottom, left=cell.border.left)\n",
        "\n",
        "# Determine the last row and column of the table\n",
        "last_row = ws.max_row\n",
        "start_col = 1  # Assuming you want the legend to start from the first column\n",
        "\n",
        "# Add legend for DUP and Split notation\n",
        "legend_dup_split = ws.cell(row=len(df_sorted)+2, column=start_col)\n",
        "legend_dup_split.value = \"*Duplicate | **Split\"\n",
        "legend_dup_split.font = Font(bold=True, name=Font_name_english, size=Font_size_english)\n",
        "legend_dup_split.alignment = alignment_center_wrap\n",
        "legend_dup_split.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for - לא בוצעה אנליזה\n",
        "legend_dup_split = ws.cell(row=len(df_sorted)+3, column=start_col)\n",
        "legend_dup_split.value = \" - לא בוצעה אנליזה\"\n",
        "legend_dup_split.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_dup_split.alignment = alignment_center_wrap\n",
        "legend_dup_split.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for ' -- לא קיים ערך סף\n",
        "legend_dup_split = ws.cell(row=len(df_sorted)+4, column=start_col)\n",
        "legend_dup_split.value = \" -- לא קיים ערך סף\"\n",
        "legend_dup_split.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_dup_split.alignment = alignment_center_wrap\n",
        "legend_dup_split.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for \"חורג מערך VSL\"\n",
        "legend_vsl_cell = ws.cell(row=len(df_sorted)+5 , column=start_col)\n",
        "legend_vsl_cell.value = \"חורג מערך VSL\"\n",
        "legend_vsl_cell.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_vsl_cell.alignment = alignment_center_wrap\n",
        "legend_vsl_cell.fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
        "legend_vsl_cell.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for \"חורג מערך TIER 1\"\n",
        "legend_tier1_cell = ws.cell(row=len(df_sorted)+6 , column=start_col)\n",
        "legend_tier1_cell.value = \"חורג מערך TIER 1\"\n",
        "legend_tier1_cell.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_tier1_cell.alignment = alignment_center_wrap\n",
        "legend_tier1_cell.fill = PatternFill(start_color=\"F7C7AC\", end_color=\"F7C7AC\", fill_type=\"solid\")\n",
        "legend_tier1_cell.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Add legend for \"ערך גילוי גדול מערך סף\"\n",
        "legend_gray_cell = ws.cell(row=len(df_sorted)+7, column=start_col)\n",
        "legend_gray_cell.value = \"ערך סף גילוי מעבדה גדול מערך סף VSL\"\n",
        "legend_gray_cell.font = Font(bold=True, name=Font_name_hebrew, size=Font_size_hebrew)\n",
        "legend_gray_cell.alignment = alignment_center_wrap\n",
        "legend_gray_cell.fill = gray_fill\n",
        "legend_gray_cell.border = Border(\n",
        "    left=Side(style=\"thin\"), right=Side(style=\"thin\"), top=Side(style=\"thin\"), bottom=Side(style=\"thin\")\n",
        ")\n",
        "\n",
        "# Save the final Excel file with the provided name\n",
        "file_path = f'{file_name}.xlsx'\n",
        "wb.save(file_path)\n",
        "\n",
        "# Download the Excel file\n",
        "files.download(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-S3t7RXOXGZY",
        "outputId": "d8d21b37-80af-4155-bf03-fd491198c76c",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-4094318418>:464: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  rows_to_sort['name_prefix'] = rows_to_sort['name'].str.extract(r'([^-\\d]+)', expand=False)\n",
            "<ipython-input-17-4094318418>:465: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  rows_to_sort['name_number'] = rows_to_sort['name'].str.extract(r'(\\d+\\.\\d+|\\d+)', expand=False).astype(float)\n",
            "<ipython-input-17-4094318418>:500: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['TPH DRO'][1] = \"C10-C40\"\n",
            "<ipython-input-17-4094318418>:501: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['TPH ORO'][1] = \"C10-C40\"\n",
            "<ipython-input-17-4094318418>:502: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['Total TPH'][1] = \"C10-C40\"\n",
            "<ipython-input-17-4094318418>:533: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['MTBE'][1] = '1634-04-4'\n",
            "<ipython-input-17-4094318418>:534: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['Benzene'][1] = '71-43-2'\n",
            "<ipython-input-17-4094318418>:535: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['Toluene'][1] = '108-88-3'\n",
            "<ipython-input-17-4094318418>:536: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['Ethylbenzene'][1] = '100-41-4'\n",
            "<ipython-input-17-4094318418>:537: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df_sorted['Xylene'][1] = '95-47-6'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9146f893-cb23-498c-b35f-c9ff53b8c53a\", \"\\u05d2\\u05dc \\u05e0\\u05ea\\u05e0\\u05d9\\u05d4.xlsx\", 11762)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}